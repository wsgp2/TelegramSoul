#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ChatGPT Telegram Chat Analyzer

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API (gpt-4o-mini –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, gpt-4o –¥–ª—è –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏) –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram —á–∞—Ç–æ–≤,
–≤—ã—è–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º, —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π.

–û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
"""

import os
import json
import logging
import asyncio
import time
from datetime import datetime
import pandas as pd
import numpy as np
import httpx
# –ò–º–ø–æ—Ä—Ç—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —É–±—Ä–∞–Ω—ã –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
from tqdm import tqdm
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
import difflib
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', f'chatgpt_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'))
    ]
)

logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MESSAGES_DIR = os.path.join(BASE_DIR, 'data', 'messages')
OUTPUT_DIR = os.path.join(BASE_DIR, 'data', 'reports')
VISUALIZATION_DIR = os.path.join(OUTPUT_DIR, 'visualizations')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
for directory in [MESSAGES_DIR, OUTPUT_DIR, VISUALIZATION_DIR, LOGS_DIR]:
    os.makedirs(directory, exist_ok=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI API
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')
MAX_TOKENS = 32000  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è gpt-4o-mini


class ChatGPTAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Telegram-—á–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT (gpt-4)
    """
    
    def __init__(self, api_key=None, model="gpt-4o-mini", api_keys=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            api_key (str): API –∫–ª—é—á OpenAI. –ï—Å–ª–∏ None, –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OPENAI_API_KEY –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            model (str): –ú–æ–¥–µ–ª—å OpenAI –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            api_keys (list): –°–ø–∏—Å–æ–∫ API –∫–ª—é—á–µ–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö API –∫–ª—é—á–µ–π
        if api_keys and isinstance(api_keys, list):
            self.api_keys = api_keys
            self.api_key = api_keys[0]  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª—é—á
        else:
            self.api_key = api_key or OPENAI_API_KEY
            self.api_keys = [self.api_key] if self.api_key else []
        
        if not self.api_keys:
            raise ValueError("API –∫–ª—é—á OpenAI –Ω–µ —É–∫–∞–∑–∞–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –∫–ª—é—á –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞")
        
        self.model = model
        self.messages_dir = MESSAGES_DIR
        self.output_dir = OUTPUT_DIR
        self.visualization_dir = VISUALIZATION_DIR
        self.client = httpx.AsyncClient(timeout=60.0)
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ChatGPT-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –º–æ–¥–µ–ª—å—é {model}")
        logger.info(f"üöÄ –î–æ—Å—Ç—É–ø–Ω–æ {len(self.api_keys)} API –∫–ª—é—á–µ–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
    async def load_messages_from_optimized_format(self, directory=None) -> List[Dict]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        
        Args:
            directory (str, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        directory = directory or self.messages_dir
        all_messages = []
        
        try:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            files = [f for f in os.listdir(directory) if f.startswith('all_messages_') and f.endswith('.json')]
            if not files:
                logger.error("–§–∞–π–ª—ã —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫.")
                return []
            
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
            latest_file = sorted(files)[-1]
            file_path = os.path.join(directory, latest_file)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑: {latest_file}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∞: {'messages': [...]} –∏ –ø—Ä–æ—Å—Ç–æ [...]
            if isinstance(data, dict) and 'messages' in data:
                messages = data.get('messages', [])
                metadata = data.get('metadata', {})
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π (—Ñ–æ—Ä–º–∞—Ç –æ–±—ä–µ–∫—Ç)")
                logger.info(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata.get('total_chats', 0)} —á–∞—Ç–æ–≤, —Å–æ–±—Ä–∞–Ω–æ {metadata.get('collection_date', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                return messages
            elif isinstance(data, list):
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å–æ–æ–±—â–µ–Ω–∏–π (—Ñ–æ—Ä–º–∞—Ç –º–∞—Å—Å–∏–≤)")
                return data
            else:
                logger.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö")
                return []
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
            return await self.load_messages_from_dir_legacy(directory)
    
    async def load_messages_from_dir_legacy(self, directory=None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (fallback)"""
        directory = directory or self.messages_dir
        all_messages = []
        user_ids = []
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–ø–∞–ø–æ–∫) –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏  
        try:
            user_ids = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]
        except FileNotFoundError:
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []
            
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        for user_id in tqdm(user_ids, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —á–∞—Ç–æ–≤"):
            messages_file = os.path.join(directory, user_id, "messages.json")
            try:
                with open(messages_file, 'r', encoding='utf-8') as f:
                    user_messages = json.load(f)
                    all_messages.extend(user_messages)
            except FileNotFoundError:
                logger.warning(f"–§–∞–π–ª —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            except json.JSONDecodeError:
                logger.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π JSON —Ñ–æ—Ä–º–∞—Ç –≤ —Ñ–∞–π–ª–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        return all_messages
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥
    async def load_messages_from_dir(self, directory=None) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø—Ä–æ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –∑–∞—Ç–µ–º legacy)"""
        return await self.load_messages_from_optimized_format(directory)
    
    def prepare_messages_for_analysis(self, messages: List[Dict], sample_size=None) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –≤—ã–±–∏—Ä–∞—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —É–¥–∞–ª—è—è —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
        Args:
            messages (List[Dict]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            sample_size (int, optional): –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            
        Returns:
            List[str]: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        text_messages = []
        for msg in messages:
            if 'content' in msg and isinstance(msg['content'], str) and len(msg['content'].strip()) > 10:
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                content = msg['content']
                if not content.startswith('/') and not content.startswith('@'):
                    text_messages.append(content)
            elif 'text' in msg and isinstance(msg['text'], str) and len(msg['text'].strip()) > 10:
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                text = msg['text']
                if not text.startswith('/') and not text.startswith('@'):
                    text_messages.append(text)
            elif 'message' in msg and isinstance(msg['message'], str) and len(msg['message'].strip()) > 2:  
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                message = msg['message']
                if not message.startswith('/') and not message.startswith('@'):
                    text_messages.append(message)
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω sample_size, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
        if sample_size and len(text_messages) > sample_size:
            np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            indices = np.random.choice(len(text_messages), sample_size, replace=False)
            text_messages = [text_messages[i] for i in indices]
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(text_messages)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return text_messages
    
    async def call_openai_api(self, messages, temperature=0.3):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç OpenAI API —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π API –∫–ª—é—á)
        
        Args:
            messages (List[Dict]): –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è API –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "...", "content": "..."}]
            temperature (float): –ü–∞—Ä–∞–º–µ—Ç—Ä temperature –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            Dict: –û—Ç–≤–µ—Ç –æ—Ç API
        """
        return await self.call_openai_api_with_key(messages, self.api_key, temperature)
    
    async def call_openai_api_with_key(self, messages, api_key, temperature=0.3):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç OpenAI API —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º API –∫–ª—é—á–æ–º
        
        Args:
            messages (List[Dict]): –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è API –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "...", "content": "..."}]
            api_key (str): –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π API –∫–ª—é—á –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            temperature (float): –ü–∞—Ä–∞–º–µ—Ç—Ä temperature –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            Dict: –û—Ç–≤–µ—Ç –æ—Ç API
        """
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4000
        }
        
        try:
            response = await self.client.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
            raise
            
    async def analyze_topics_fast(self, text_messages: List[str], max_tokens_per_chunk: int = 8000, checkpoint_base: str = None):
        """
        –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (–≠–¢–ê–ü 1)
        
        Args:
            text_messages (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            max_tokens_per_chunk (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ API
            checkpoint_base (str): –ë–∞–∑–æ–≤–æ–µ –∏–º—è –¥–ª—è checkpoint —Ñ–∞–π–ª–æ–≤
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON (10-15 —Ç–µ–º)
        """
        logger.info("üöÄ –≠–¢–ê–ü 1: –ù–∞—á–∏–Ω–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º...")
        
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –º–∞–ª–æ, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å—Ä–∞–∑—É
        if len('\n'.join(text_messages)) < 10000:  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
            messages_text = '\n'.join(text_messages)
            prompt = FAST_TOPIC_ANALYSIS_PROMPT.format(messages=messages_text)
            
            messages_for_api = [
                {"role": "system", "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º."},
                {"role": "user", "content": prompt}
            ]
            
            try:
                response = await self.call_openai_api(messages_for_api, temperature=0.2)
                content = response['choices'][0]['message']['content']
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
                return self.extract_json_from_text(content)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–º: {e}")
                return {"topics": []}
        
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –º–Ω–æ–≥–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        chunk_size = len(text_messages) // ((len('\n'.join(text_messages)) // max_tokens_per_chunk) + 1)
        chunk_messages = ['\n'.join(text_messages[i:i + chunk_size]) for i in range(0, len(text_messages), chunk_size)]
        
        logger.info(f"–°–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(chunk_messages)} —á–∞—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # üîÑ –ü–†–û–í–ï–†–Ø–ï–ú CHECKPOINT
        all_topics = []
        start_chunk = 0
        processed_results = []
        
        if checkpoint_base:
            checkpoint_data = self.load_checkpoint(checkpoint_base)
            if checkpoint_data and checkpoint_data.get('total_chunks') == len(chunk_messages):
                processed_results = checkpoint_data.get('chunk_results', [])
                start_chunk = checkpoint_data.get('last_processed_chunk', 0) + 1
                logger.info(f"üîÑ –í–û–°–°–¢–ê–ù–ê–í–õ–ò–í–ê–ï–ú –∞–Ω–∞–ª–∏–∑ —Å —á–∞—Å—Ç–∏ {start_chunk + 1} –∏–∑ {len(chunk_messages)}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
                for result in processed_results:
                    if isinstance(result, list):
                        all_topics.extend(result)
                        
            elif checkpoint_data:
                logger.warning("‚ö†Ô∏è Checkpoint –Ω–∞–π–¥–µ–Ω, –Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç. –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ.")
                self.cleanup_checkpoint(checkpoint_base)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —á–∞—Å—Ç–∏
        remaining_chunks = chunk_messages[start_chunk:]
        if not remaining_chunks:
            logger.info("‚úÖ –í—Å–µ —á–∞—Å—Ç–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –ó–∞–≤–µ—Ä—à–∞–µ–º –∞–Ω–∞–ª–∏–∑...")
            aggregated_topics = self._aggregate_similar_topics(all_topics)
            if checkpoint_base:
                self.cleanup_checkpoint(checkpoint_base)
            return {"topics": aggregated_topics}
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ chunk'–∞
        async def process_chunk(chunk_text, chunk_index, api_key):
            real_index = start_chunk + chunk_index
            logger.info(f"üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å {real_index+1} –∏–∑ {len(chunk_messages)} (API –∫–ª—é—á #{self.api_keys.index(api_key)+1})")
            
            messages = [
                {"role": "system", "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º."},
                {"role": "user", "content": FAST_TOPIC_ANALYSIS_PROMPT.format(messages=chunk_text)}
            ]
            
            try:
                response = await self.call_openai_api_with_key(messages, api_key, temperature=0.2)
                content = response['choices'][0]['message']['content']
                
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
                chunk_topics = self.extract_json_from_text(content)
                topics_found = chunk_topics.get("topics", [])
                logger.info(f"‚úÖ –ß–∞—Å—Ç—å {real_index+1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞, –Ω–∞–π–¥–µ–Ω–æ {len(topics_found)} —Ç–µ–º")
                return topics_found
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —á–∞—Å—Ç–∏ {real_index+1}: {e}")
                return []
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = []
        for i, chunk in enumerate(remaining_chunks):
            # –í—ã–±–∏—Ä–∞–µ–º API –∫–ª—é—á –ø–æ –∫—Ä—É–≥—É
            api_key = self.api_keys[i % len(self.api_keys)]
            task = process_chunk(chunk, i, api_key)
            tasks.append(task)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≥—Ä—É–ø–ø–∞–º–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É API –∫–ª—é—á–µ–π
        batch_size = len(self.api_keys)
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i+batch_size]
            batch_start_idx = start_chunk + i
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≥—Ä—É–ø–ø—ã {i//batch_size+1}, –∑–∞–¥–∞—á: {len(batch)} (—á–∞—Å—Ç–∏ {batch_start_idx+1}-{batch_start_idx+len(batch)})")
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –≥—Ä—É–ø–ø–µ
            results = await asyncio.gather(*batch, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for j, result in enumerate(results):
                current_chunk_idx = batch_start_idx + j
                
                if isinstance(result, list):
                    all_topics.extend(result)
                    processed_results.append(result)
                    
                    # üíæ –°–û–•–†–ê–ù–Ø–ï–ú CHECKPOINT –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
                    if checkpoint_base:
                        self.save_checkpoint(processed_results, current_chunk_idx, len(chunk_messages), checkpoint_base)
                        
                elif isinstance(result, Exception):
                    logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–µ —á–∞—Å—Ç–∏ {current_chunk_idx+1}: {result}")
                    processed_results.append([])  # –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
                
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã
        aggregated_topics = self._aggregate_similar_topics(all_topics)
        
        # üóëÔ∏è –£–¥–∞–ª—è–µ–º checkpoint –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if checkpoint_base:
            self.cleanup_checkpoint(checkpoint_base)
        
        logger.info(f"üéØ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à–µ–Ω. –í—ã—è–≤–ª–µ–Ω–æ {len(aggregated_topics)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º")
        return {"topics": aggregated_topics}

    async def analyze_final_monetization_psychology(self, aggregated_topics: Dict):
        """
        –ì–ª—É–±–æ–∫–∏–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ (–≠–¢–ê–ü 2)
        
        Args:
            aggregated_topics (Dict): –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –≤—Å–µ—Ö —á–∞—Ç–æ–≤
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è + –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è)
        """
        logger.info("üí∞ üß† –≠–¢–ê–ü 2: –ù–∞—á–∏–Ω–∞–µ–º –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏...")
        
        if not aggregated_topics or not aggregated_topics.get('topics'):
            logger.error("–ù–µ—Ç —Ç–µ–º –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return {"expertise_analysis": [], "psychological_analysis": {}}
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        import json
        topics_json = json.dumps(aggregated_topics, ensure_ascii=False, indent=2)
        prompt = FINAL_ANALYSIS_PROMPT.format(aggregated_topics=topics_json)
        
        messages_for_api = [
            {"role": "system", "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —Å—Ç—Ä–∞—Ç–µ–≥ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ—É—á."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = await self.call_openai_api(messages_for_api, temperature=0.3)
            content = response['choices'][0]['message']['content']
            result = self.extract_json_from_text(content)
            
            logger.info("üéØ –≠–¢–ê–ü 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return {"expertise_analysis": [], "psychological_analysis": {}}
    
    def extract_json_from_text(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        import re  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º re –≤–Ω—É—Ç—Ä–∏ –º–µ—Ç–æ–¥–∞
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        os.makedirs(LOGS_DIR, exist_ok=True)
        log_file_path = os.path.join(LOGS_DIR, f"json_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(f"--- –¢–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON ---\n{text}\n--- –ö–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞ ---")
        
        logger.info(f"–¢–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {log_file_path}")
        logger.info(f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON:\n{text[:500]}")
        logger.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON:\n{text[-500:] if len(text) > 500 else text}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 0: –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–±–µ—Ä–Ω—É—Ç –≤ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫ –∫–æ–¥–∞ ```json ... ```
            code_block_pattern = r'```(?:json)?(.*?)```'
            code_blocks = re.findall(code_block_pattern, text, re.DOTALL)
            
            if code_blocks:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –±–ª–æ–∫ –∫–æ–¥–∞
                json_text = code_blocks[0].strip()
                logger.info(f"–ù–∞–π–¥–µ–Ω JSON –≤ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–µ –∫–æ–¥–∞, –¥–ª–∏–Ω–∞: {len(json_text)}")
                try:
                    return json.loads(json_text)
                except json.JSONDecodeError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –∏–∑ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–∞: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–∞: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}, –ø–æ–∑–∏—Ü–∏—è: {e.pos}, —Å—Ç—Ä–æ–∫–∞: {text[max(0, e.pos-50):e.pos+50] if e.pos < len(text) else '–∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞'}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
        try:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏
            open_braces = [m.start() for m in re.finditer('{', text)]
            close_braces = [m.start() for m in re.finditer('}', text)]
            
            if open_braces and close_braces:
                # –ù–∞–π–¥–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–∞—Ä—É —Å–∫–æ–±–æ–∫, —É—á–∏—Ç—ã–≤–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                start_index = open_braces[0]  # –ü–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
                
                # –°—á–∏—Ç–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å —Å–∫–æ–±–æ–∫
                depth = 0
                for i in range(len(text)):
                    if i in open_braces:
                        depth += 1
                    elif i in close_braces:
                        depth -= 1
                        if depth == 0 and i > start_index:
                            # –ù–∞—à–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
                            end_index = i
                            json_str = text[start_index:end_index+1]
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏: {start_index} - {end_index}")
                            return json.loads(json_str)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–∫–æ–±–æ–∫ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–π –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ñ–∏–≥—É—Ä–Ω–æ–π —Å–∫–æ–±–∫–∏
        try:
            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é —Ñ–∏–≥—É—Ä–Ω—É—é —Å–∫–æ–±–∫—É
            start_index = text.find('{')
            end_index = text.rfind('}')
            
            if start_index >= 0 and end_index > start_index:
                json_str = text[start_index:end_index+1]
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: –Ω–∞—á–∞–ª–æ {start_index}, –∫–æ–Ω–µ—Ü {end_index}")
                logger.info(f"–ò–∑–≤–ª–µ–∫–∞–µ–º—ã–π JSON: {json_str[:100]}...{json_str[-100:] if len(json_str) > 200 else json_str}")
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: {e}")
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON –æ–±—ä–µ–∫—Ç–∞
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            json_pattern = r'\{[\s\S]*?\}'
            matches = list(re.finditer(json_pattern, text))
            
            if matches:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
                candidate = None
                max_length = 0
                
                for match in matches:
                    json_str = match.group(0)
                    if len(json_str) > max_length:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON
                            json.loads(json_str)
                            candidate = json_str
                            max_length = len(json_str)
                        except:
                            continue
                
                if candidate:
                    logger.info(f"–ù–∞–π–¥–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è")
                    return json.loads(candidate)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: {e}")
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ –≤ JSON
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ
            fixed_text = text.replace("'", '"')
            return json.loads(fixed_text)
        except json.JSONDecodeError:
            pass
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 6: –ü–æ–∏—Å–∫ —Å—Ç—Ä–æ–∫–∏ "topics": –∏ –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ–±—ä–µ–∫—Ç–∞
        try:
            topics_index = text.find('"topics":')
            if topics_index > 0:
                # –ò—â–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –ø–µ—Ä–µ–¥ "topics"
                open_brace_index = text.rfind('{', 0, topics_index)
                if open_brace_index >= 0:
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                    json_str = text[open_brace_index:]
                    depth = 0
                    for i, char in enumerate(json_str):
                        if char == '{':
                            depth += 1
                        elif char == '}':
                            depth -= 1
                            if depth == 0:
                                json_str = json_str[:i+1]
                                logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω JSON –æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º 'topics': {json_str[:100]}...{json_str[-100:] if len(json_str) > 200 else json_str}")
                                return json.loads(json_str)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –ø–æ –∫–ª—é—á—É 'topics': {e}")
        
        # –ï—Å–ª–∏ –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —à–∞–±–ª–æ–Ω")
        return {"topics": [{
            "name": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
            "keywords": ["–æ—à–∏–±–∫–∞"],
            "percentage": 100,
            "sentiment": "neutral",
            "description": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—Ç–≤–µ—Ç–∞ API."
        }]}
    
    def _aggregate_similar_topics(self, topics: List[Dict]) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—Ö–æ–∂–∏—Ö —Ç–µ–º —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
        
        Args:
            topics (List[Dict]): –°–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º
        """
        if not topics:
            return []
            
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç—Ä–æ–∫
        import difflib
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å —Å–∞–º—ã—Ö –∑–Ω–∞—á–∏–º—ã—Ö
        sorted_topics = sorted(topics, key=lambda x: x.get('percentage', 0), reverse=True)
        
        # –°–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø —Ç–µ–º —Å–æ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        topic_groups = {
            '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã_–∏_–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏': {
                'patterns': ['–∫—Ä–∏–ø—Ç–æ', '–±–∏—Ç–∫–æ–∏–Ω', '–∞–ª—å—Ç–∫–æ–∏–Ω', '–∏–Ω–≤–µ—Å—Ç', '—Ç–æ—Ä–≥–æ–≤', '–±–ª–æ–∫—á–µ–π–Ω', '–¥–µ–Ω—å–≥–∏', '—Ñ–∏–Ω–∞–Ω—Å'],
                'merged_topics': [],
                'final_name': '–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'
            },
            '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è_–∏_—Ç—É—Ä–∏–∑–º': {
                'patterns': ['–ø—É—Ç–µ—à–µ—Å—Ç–≤', '—Ç—É—Ä–∏–∑–º', '–ø–æ–µ–∑–¥–∫', '–æ—Ç–¥—ã—Ö', '–±–∞–ª–∏', '—Ç–∞–π', '—Ñ–∏–ª–∏–ø–ø–∏–Ω'],
                'merged_topics': [],
                'final_name': '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è –∏ —Ç—É—Ä–∏–∑–º'
            },
            '—Å–æ–±—ã—Ç–∏—è_–∏_–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è': {
                'patterns': ['—Å–æ–±—ã—Ç–∏', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç', '–≤—Å—Ç—Ä–µ—á', '—Å–æ–∑–≤–æ–Ω', '–Ω–µ—Ç–≤–æ—Ä–∫', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '–±–∞—Ç–ª'],
                'merged_topics': [],
                'final_name': '–°–æ–±—ã—Ç–∏—è –∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è'
            },
            '–ª–∏—á–Ω–æ–µ_—Ä–∞–∑–≤–∏—Ç–∏–µ': {
                'patterns': ['—Ä–∞–∑–≤–∏—Ç', '–∫—É—Ä—Å', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–æ–±—É—á–µ–Ω', '–∞–ª—å—Ñ–∞', '–∫–æ—É—á', '—Å–µ–º–∏–Ω–∞—Ä'],
                'merged_topics': [],
                'final_name': '–õ–∏—á–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ'
            },
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_–≤–æ–ø—Ä–æ—Å—ã': {
                'patterns': ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç', '—Å–æ—Ñ—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω', '–±–æ—Ç'],
                'merged_topics': [],
                'final_name': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã'
            }
        }
        
        # –ù–µ—Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        ungrouped_topics = []
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—ã –ø–æ –≥—Ä—É–ø–ø–∞–º
        for topic in sorted_topics:
            topic_name = topic.get('name', '').lower()
            keywords_text = ' '.join(topic.get('keywords', [])).lower()
            description_text = topic.get('description', '').lower()
            full_text = f"{topic_name} {keywords_text} {description_text}"
            
            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é –≥—Ä—É–ø–ø—É
            assigned = False
            for group_key, group_info in topic_groups.items():
                patterns = group_info['patterns']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –≥—Ä—É–ø–ø—ã
                matches = sum(1 for pattern in patterns if pattern in full_text)
                
                # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ 2+ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–ª–∏ 1 —Å–∏–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
                if matches >= 2 or any(pattern in topic_name for pattern in patterns):
                    group_info['merged_topics'].append(topic)
                    assigned = True
                    break
            
            if not assigned:
                ungrouped_topics.append(topic)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        final_topics = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã
        for group_key, group_info in topic_groups.items():
            if group_info['merged_topics']:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–º—ã –≥—Ä—É–ø–ø—ã
                total_percentage = sum(t.get('percentage', 0) for t in group_info['merged_topics'])
                all_keywords = []
                all_descriptions = []
                sentiments = []
                
                for topic in group_info['merged_topics']:
                    all_keywords.extend(topic.get('keywords', []))
                    all_descriptions.append(topic.get('description', ''))
                    sentiments.append(topic.get('sentiment', 'neutral'))
                
                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –±–µ—Ä–µ–º —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ
                unique_keywords = list(dict.fromkeys(all_keywords))[:8]
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
                for s in sentiments:
                    sentiment_counts[s] = sentiment_counts.get(s, 0) + 1
                final_sentiment = max(sentiment_counts, key=sentiment_counts.get)
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                best_description = max(all_descriptions, key=len) if all_descriptions else ""
                
                merged_topic = {
                    'name': group_info['final_name'],
                    'keywords': unique_keywords,
                    'percentage': total_percentage,
                    'sentiment': final_sentiment,
                    'description': best_description,
                    'merged_from': len(group_info['merged_topics'])
                }
                
                final_topics.append(merged_topic)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        final_topics.extend(ungrouped_topics)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–∞
        final_topics = sorted(final_topics, key=lambda x: x.get('percentage', 0), reverse=True)
        
        return final_topics[:7]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-7 —Ç–µ–º
        
    async def assess_commercial_potential(self, topics: Dict):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–º —Å –ø–æ–º–æ—â—å—é ChatGPT
        
        Args:
            topics (Dict): JSON –æ–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ç–µ–º...")
        
        if not topics or not topics.get('topics'):
            logger.warning("–ù–µ—Ç —Ç–µ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞")
            return {"commercial_assessment": []}
        
        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ChatGPT –∞–Ω–∞–ª–∏–∑–∞
        topics_for_analysis = []
        for topic in topics.get('topics', []):
            topics_for_analysis.append({
                "name": topic.get('name', ''),
                "keywords": topic.get('keywords', []),
                "percentage": topic.get('percentage', 0),
                "sentiment": topic.get('sentiment', 'neutral'),
                "description": topic.get('description', '')
            })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è ChatGPT
        prompt = f"""–û—Ü–µ–Ω–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ç–µ–º –∏–∑ –ø–µ—Ä–µ–ø–∏—Å–æ–∫ –∏ –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞—Ä–∞–±–æ—Ç–∫—É.

–¢–ï–ú–´: {json.dumps(topics_for_analysis, ensure_ascii=False)}

–î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã —É–∫–∞–∂–∏:
1. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª (high/medium/low)
2. –†–µ–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥ –≤ –º–µ—Å—è—Ü
3. –°–ø–æ—Å–æ–± –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
4. –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è  
5. –°—Ç–∞—Ä—Ç–æ–≤—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã
6. –ü–µ—Ä–≤—ã–µ —à–∞–≥–∏

JSON —Ñ–æ—Ä–º–∞—Ç:
{{
  "commercial_assessment": [
    {{
      "topic_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ",
      "commercial_potential": "high/medium/low", 
      "realistic_revenue": "10,000-50,000 —Ä—É–±/–º–µ—Å",
      "monetization_methods": [{{
        "method": "—Å–ø–æ—Å–æ–± –∑–∞—Ä–∞–±–æ—Ç–∫–∞",
        "description": "–æ–ø–∏—Å–∞–Ω–∏–µ",
        "target_audience": "–∞—É–¥–∏—Ç–æ—Ä–∏—è",
        "startup_cost": "0-10,000 —Ä—É–±",
        "time_to_profit": "1-3 –º–µ—Å—è—Ü–∞",
        "success_probability": "60-80%",
        "first_steps": ["—à–∞–≥1", "—à–∞–≥2"]
      }}],
      "why_this_person": "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    }}
  ]
}}"""

        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ ChatGPT
            messages = [
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ä—ã–Ω–∫–∞ –∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏. –î–∞–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."},
                {"role": "user", "content": prompt}
            ]
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—É—é –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å GPT-4o –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            response = await self.call_openai_api_with_model(messages, model="gpt-4o", temperature=0.1)
            
            if response and response.get('choices'):
                content = response['choices'][0]['message']['content']
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                commercial_data = self.extract_json_from_text(content)
                
                if commercial_data and isinstance(commercial_data, dict):
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ {len(commercial_data.get('commercial_assessment', []))} —Ç–µ–º")
                    return commercial_data
                else:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ ChatGPT –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏")
                    return self._fallback_commercial_assessment(topics)
            else:
                logger.error("–ù–µ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç ChatGPT –¥–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏")
                return self._fallback_commercial_assessment(topics)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞: {e}")
            return self._fallback_commercial_assessment(topics)
    
    def _fallback_commercial_assessment(self, topics: Dict):
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –µ—Å–ª–∏ ChatGPT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        assessment = []
        for topic in topics.get('topics', []):
            commercial_score = self._calculate_commercial_score(topic)
            assessment.append({
                "topic_name": topic.get('name', ''),
                "commercial_potential": commercial_score,
                "realistic_revenue": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
                "monetization_methods": [{
                    "method": "–ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞",
                    "description": self._get_commercial_assessment(commercial_score),
                    "target_audience": "–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ",
                    "startup_cost": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω",
                    "time_to_profit": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω",
                    "success_probability": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω",
                    "first_steps": ["–ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞"]
                }],
                "market_insights": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑",
                "risks": ["–ù–µ–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏"],
                "why_this_person": "–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"
            })
        
        logger.warning("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞")
        return {"commercial_assessment": assessment}
    
    def _calculate_commercial_score(self, topic: Dict) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ç–µ–º—ã"""
        keywords = topic.get('keywords', [])
        percentage = topic.get('percentage', 0)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –≤—ã—Å–æ–∫–∏–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º
        commercial_keywords = ['–¥–µ–Ω—å–≥–∏', '–±–∏–∑–Ω–µ—Å', '—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ–¥–∞–∂–∏', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–∫–∞—Ä—å–µ—Ä–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–∑–∞—Ä–∞–±–æ—Ç–æ–∫', '–¥–æ—Ö–æ–¥', '–º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ', '—Å—Ç–∞—Ä—Ç–∞–ø']
        
        score = 0
        for keyword in keywords:
            if any(comm_word in keyword.lower() for comm_word in commercial_keywords):
                score += 1
        
        if percentage > 5:
            score += 1
        elif percentage > 2:
            score += 0.5
            
        if score >= 2:
            return "high"
        elif score >= 1:
            return "medium"
        else:
            return "low"
    
    def _get_commercial_assessment(self, score: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞"""
        assessments = {
            "high": "–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏. –¢–µ–º–∞ –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –∏ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏.",
            "medium": "–°—Ä–µ–¥–Ω–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏. –í–æ–∑–º–æ–∂–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.",
            "low": "–ù–∏–∑–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏. –¢–µ–º–∞ –Ω–æ—Å–∏—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä."
        }
        return assessments.get(score, "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª")
            
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–æ–≤ —É–¥–∞–ª–µ–Ω–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
        
    def save_results_to_json(self, data: Dict, filename: str, directory: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª
        
        Args:
            data (Dict): –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filename (str): –ò–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
            directory (str, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é self.output_dir
            
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        directory = directory or self.output_dir
        os.makedirs(directory, exist_ok=True)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = os.path.join(directory, f"{filename}_{timestamp}.json")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        return filepath
        
    def generate_report(self, topics: Dict, monetization: Dict = None, business_plan: Dict = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            topics (Dict): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            monetization (Dict, optional): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            business_plan (Dict, optional): –î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω
            
        Returns:
            str: –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
        """
        report = ["# –û—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ Telegram-—á–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT\n"]
        report.append(f"*–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å —Ç–µ–º–∞–º–∏
        if topics and topics.get('topics'):
            report.append("## 1. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è\n")
            for i, topic in enumerate(topics['topics'], 1):
                sentiment_emoji = {
                    "positive": "üòä",
                    "neutral": "üòê",
                    "negative": "üòü"
                }.get(topic.get('sentiment', 'neutral'), "üòê")
                
                report.append(f"### {i}. {topic['name']} ({topic['percentage']}%) {sentiment_emoji}\n")
                report.append(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(topic['keywords'])}\n")
                report.append(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {topic['description']}\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
        if monetization and monetization.get('monetization_strategies'):
            report.append("## 2. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏\n")
            for i, strategy in enumerate(monetization['monetization_strategies'], 1):
                report.append(f"### {i}. –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–º—ã '{strategy['topic']}'\n")
                
                for j, product in enumerate(strategy['products'], 1):
                    revenue_emoji = {
                        "high": "üí∞üí∞üí∞",
                        "medium": "üí∞üí∞",
                        "low": "üí∞"
                    }.get(product.get('revenue_potential', '').lower(), "üí∞")
                    
                    complexity_emoji = {
                        "high": "üî¥",
                        "medium": "üü†",
                        "low": "üü¢"
                    }.get(product.get('implementation_complexity', '').lower(), "üü†")
                    
                    report.append(f"#### {j}. {product['name']} {revenue_emoji} {complexity_emoji}\n")
                    report.append(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {product['description']}\n")
                    report.append(f"**–ú–æ–¥–µ–ª—å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏:** {product['model']}\n")
                    report.append(f"**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {product['revenue_potential']}\n")
                    report.append(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:** {product['implementation_complexity']}\n")
                    report.append(f"**–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏:** {product['timeframe']}\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–æ–º
        if business_plan and business_plan.get('business_plan'):
            bp = business_plan['business_plan']
            report.append("## 3. –î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω\n")
            
            if bp.get('executive_summary'):
                report.append("### 3.1. –†–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞\n")
                report.append(f"**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:** {bp['executive_summary'].get('concept', '')}\n")
                report.append(f"**–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {bp['executive_summary'].get('target_audience', '')}\n")
                report.append(f"**–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:** {bp['executive_summary'].get('value_proposition', '')}\n")
            
            if bp.get('market_analysis'):
                report.append("### 3.2. –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞\n")
                report.append(f"**–†–∞–∑–º–µ—Ä —Ä—ã–Ω–∫–∞:** {bp['market_analysis'].get('market_size', '')}\n")
                
                if bp['market_analysis'].get('trends'):
                    report.append("**–¢–µ–Ω–¥–µ–Ω—Ü–∏–∏:**\n")
                    for trend in bp['market_analysis']['trends']:
                        report.append(f"- {trend}\n")
                
                if bp['market_analysis'].get('competitors'):
                    report.append("**–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã:**\n")
                    for competitor in bp['market_analysis']['competitors']:
                        report.append(f"- {competitor}\n")
            
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞ –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏...
            
        report.append("\n---\n")
        report.append("*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT Analyzer*")
        
        return '\n'.join(report)
    
    def generate_executive_summary(self, topics: Dict, commercial_assessment: Dict = None, all_topics_data: list = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤
        """
        from datetime import datetime
        
        topics_list = topics.get('topics', [])
        assessment_list = commercial_assessment.get('commercial_assessment', []) if commercial_assessment else []
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_messages = sum(chat.get('message_count', 0) for chat in (all_topics_data or []))
        total_chats = len(all_topics_data or [])
        
        # –ü–æ–ª—É—á–∞–µ–º –¢–û–ü-3 —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É
        top_topics = sorted(topics_list, key=lambda x: x.get('percentage', 0), reverse=True)[:3]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–º—ã —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º
        commercial_topics = [a for a in assessment_list if a.get('commercial_potential') in ['medium', 'high']]
        
        summary = f"""# üìä –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï
## –ê–Ω–∞–ª–∏–∑ Telegram-–ø–µ—Ä–µ–ø–∏—Å–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ò–ò

---

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%d %B %Y')}  
**–ê–Ω–∞–ª–∏—Ç–∏–∫:** TelegramSoul AI System  
**–°—Ç–∞—Ç—É—Å:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ

---

## üéØ –û–°–ù–û–í–ù–´–ï –¶–ò–§–†–´

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---------|----------|
| **–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π** | {total_messages:,} |
| **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Ç–æ–≤** | {total_chats} |
| **–í—ã—è–≤–ª–µ–Ω–æ —Ç–µ–º** | {len(topics_list)} –æ—Å–Ω–æ–≤–Ω—ã—Ö |
| **–¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞** | 95%+ (–º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∞) |

---

## üèÜ –¢–û–ü-3 –î–û–ú–ò–ù–ò–†–£–Æ–©–ò–ï –¢–ï–ú–´

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¢–û–ü-3 —Ç–µ–º—ã
        for i, topic in enumerate(top_topics, 1):
            sentiment_emoji = "üòä" if topic.get('sentiment') == 'positive' else "üòê" if topic.get('sentiment') == 'neutral' else "üòî"
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è —ç—Ç–æ–π —Ç–µ–º—ã
            commercial_potential = "–ù–∏–∑–∫–∏–π"
            for assessment in assessment_list:
                if assessment.get('topic_name') == topic.get('name'):
                    if assessment.get('commercial_potential') == 'medium':
                        commercial_potential = "‚≠ê **–°–†–ï–î–ù–ò–ô**"
                    elif assessment.get('commercial_potential') == 'high':
                        commercial_potential = "üî• **–í–´–°–û–ö–ò–ô**"
                    break
            
            summary += f"""### {i}Ô∏è‚É£ {topic.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')} ({topic.get('percentage', 0):.1f}%)
- **–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {topic.get('sentiment', 'neutral').title()} {sentiment_emoji}
- **–ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã:** {', '.join(topic.get('keywords', [])[:5])}
- **–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª:** {commercial_potential}

"""
        
        summary += """---

## üí∞ –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò

### üî• –ü–ï–†–°–ü–ï–ö–¢–ò–í–ù–´–ï –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø:

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        for i, assessment in enumerate(commercial_topics, 1):
            topic_name = assessment.get('topic_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
            realistic_revenue = assessment.get('realistic_revenue', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            methods = assessment.get('monetization_methods', [])
            if methods:
                method = methods[0]
                method_name = method.get('method', '–ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞')
                description = method.get('description', '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ')
                target_audience = method.get('target_audience', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
                startup_cost = method.get('startup_cost', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
                time_to_profit = method.get('time_to_profit', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
                
                summary += f"""{i}. **{topic_name}** 
   - **–ú–µ—Ç–æ–¥ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏:** {method_name}
   - **–û–ø–∏—Å–∞–Ω–∏–µ:** {description}
   - **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {realistic_revenue}
   - **–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {target_audience}
   - **–°—Ç–∞—Ä—Ç–æ–≤—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã:** {startup_cost}
   - **–í—Ä–µ–º—è –¥–æ –ø—Ä–∏–±—ã–ª–∏:** {time_to_profit}

"""
            else:
                summary += f"""{i}. **{topic_name}**
   - **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {realistic_revenue}
   - –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑

"""
        
        if not commercial_topics:
            summary += """‚ùå –ù–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç —Ç–µ–º—ã —Å –≤—ã—Å–æ–∫–∏–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º –Ω–µ –≤—ã—è–≤–ª–µ–Ω—ã.
üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.

"""
        
        summary += """### üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ú–û–ù–ï–¢–ò–ó–ê–¶–ò–ò:

- ‚úÖ **–ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã** –≤ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ
- ‚úÖ **–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã** –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é
- ‚úÖ **–†–µ—Ñ–µ—Ä–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã** –¥–ª—è —Ñ–∏–Ω—Ç–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
- ‚ö†Ô∏è –ò–∑–±–µ–≥–∞—Ç—å —á–∏—Å—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤

---

## üìà –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò

### –î–õ–Ø –£–í–ï–õ–ò–ß–ï–ù–ò–Ø –í–´–ë–û–†–ö–ò:
1. –†–∞—Å—à–∏—Ä–∏—Ç—å —Å–±–æ—Ä –¥–æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —á–∞—Ç–æ–≤
2. –£–≤–µ–ª–∏—á–∏—Ç—å –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞ (–±–æ–ª—å—à–µ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —á–∞—Ç)
3. –î–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

### –î–õ–Ø –ú–û–ù–ï–¢–ò–ó–ê–¶–ò–ò:
1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö
2. –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º —Ç–µ–º–∞–º
3. –°–æ–∑–¥–∞—Ç—å —Ä–µ—Ñ–µ—Ä–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤

---

**üìß –í–æ–ø—Ä–æ—Å—ã:** TelegramSoul AI System  
**üìÅ –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** `data/reports/` –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è

*–≠—Ç–æ—Ç –æ—Ç—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –Ω–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ*
"""
        
        return summary
    
    def create_simple_summary(self, topics: Dict) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –≤–º–µ—Å—Ç–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        
        Args:
            topics (Dict): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            
        Returns:
            str: –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ
        """
        if not topics or not topics.get('topics'):
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ"
        
        topic_data = topics.get('topics', [])
        
        summary_lines = [
            "=== –†–ï–ó–Æ–ú–ï –ê–ù–ê–õ–ò–ó–ê –¢–ï–ú ===\n",
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(topic_data)} –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º\n"
        ]
        
        # –¢–æ–ø-3 —Ç–µ–º—ã
        summary_lines.append("üìä –¢–û–ü-3 –ù–ê–ò–ë–û–õ–ï–ï –û–ë–°–£–ñ–î–ê–ï–ú–´–ï –¢–ï–ú–´:")
        for i, topic in enumerate(topic_data[:3], 1):
            summary_lines.append(f"{i}. {topic.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')} - {topic.get('percentage', 0)}%")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        all_keywords = []
        for topic in topic_data:
            all_keywords.extend(topic.get('keywords', []))
        
        if all_keywords:
            top_keywords = list(set(all_keywords))[:10]
            summary_lines.append(f"\nüîë –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {', '.join(top_keywords)}")
        
        # –û–±—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiments = [topic.get('sentiment', 'neutral') for topic in topic_data]
        positive_count = sentiments.count('positive')
        negative_count = sentiments.count('negative')
        
        if positive_count > negative_count:
            summary_lines.append("\nüòä –û–ë–©–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è")
        elif negative_count > positive_count:
            summary_lines.append("\nüòü –û–ë–©–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è")
        else:
            summary_lines.append("\nüòê –û–ë–©–ê–Ø –¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è")
        
        return "\n".join(summary_lines)
        
    async def run_fast_topic_analysis(self, chat_name: str, messages_limit: int = None, save_results: bool = True):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞—Ç–∞ (–≠–¢–ê–ü 1)
        
        Args:
            chat_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            messages_limit (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            save_results (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º (10-15 —Ç–µ–º)
        """
        logger.info(f"üöÄ –≠–¢–ê–ü 1: –ó–∞–ø—É—Å–∫–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –¥–ª—è —á–∞—Ç–∞ '{chat_name}'")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = await self.load_messages_from_dir(directory=os.path.join(self.messages_dir, chat_name))
        if not messages:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —á–∞—Ç–∞ '{chat_name}'")
            return {"topics": []}
            
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —á–∞—Ç–∞ '{chat_name}'")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º
        text_messages = self.prepare_messages_for_analysis(messages, sample_size=messages_limit)
        checkpoint_base = f"{chat_name}_fast_topics" if save_results else None
        
        topics_result = await self.analyze_topics_fast(text_messages, checkpoint_base=checkpoint_base)
        
        if not topics_result:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º")
            return {"topics": []}
            
        logger.info(f"üéØ –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞—Ç–∞ '{chat_name}' –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if save_results:
            self.save_results_to_json(topics_result, f"{chat_name}_fast_topics_analysis")
            logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è —á–∞—Ç–∞ '{chat_name}'")
        
        return topics_result
    
    async def run_complete_soul_analysis(self, all_topics_data: Dict, user_name: str = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", save_results: bool = True):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ç–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≠–¢–ê–ü 2)
        
        Args:
            all_topics_data (Dict): –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –≤—Å–µ—Ö —á–∞—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_name (str): –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –æ—Ç—á–µ—Ç–∞
            save_results (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
            
        Returns:
            Dict: –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ (—Ç–µ–º—ã + –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è + –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è)
        """
        logger.info(f"üåü –≠–¢–ê–ü 2: –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π Soul Analysis –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è '{user_name}'")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏
        deep_analysis_result = await self.analyze_final_monetization_psychology(all_topics_data)
        
        if not deep_analysis_result:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑")
            deep_analysis_result = {"expertise_analysis": [], "psychological_analysis": {}}
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        complete_result = {
            "topics": all_topics_data.get('topics', []),
            "expertise_analysis": deep_analysis_result.get('expertise_analysis', []),
            "psychological_analysis": deep_analysis_result.get('psychological_analysis', {})
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if save_results:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.save_results_to_json(complete_result, f"{user_name}_complete_soul_analysis")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç—á–µ—Ç
            beautiful_report = self.generate_beautiful_soul_report(complete_result, user_name)
            
            report_path = os.path.join(self.output_dir, f"{user_name}_SOUL_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(beautiful_report)
                
            logger.info(f"üåü Complete Soul Analysis Report —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")
            complete_result['report_path'] = report_path
        
        logger.info(f"üöÄ –ü–æ–ª–Ω—ã–π Soul Analysis –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è '{user_name}' –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return complete_result

    def generate_comprehensive_client_report(self, topics: Dict, commercial_assessment: Dict = None, chat_name: str = "–ö–ª–∏–µ–Ω—Ç") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–¥–∏–Ω—ã–π comprehensive –æ—Ç—á–µ—Ç –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —Å–æ –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        
        Args:
            topics (Dict): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            commercial_assessment (Dict): –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
            chat_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞/–∫–ª–∏–µ–Ω—Ç–∞
            
        Returns:
            str: –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
        """
        current_date = datetime.now().strftime('%d.%m.%Y')
        
        report = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –≤–≤–µ–¥–µ–Ω–∏–µ
        report.append(f"""# üöÄ –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó TELEGRAM-–ü–ï–†–ï–ü–ò–°–û–ö
## –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è {chat_name}

---

**üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {current_date}  
**ü§ñ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞:** TelegramSoul AI  
**‚ú® –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è:** ChatGPT + –≥–ª—É–±–æ–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

---

## üéØ –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï

–ü—Ä–æ–≤–µ–¥–µ–Ω –ø–æ–ª–Ω—ã–π –ò–ò-–∞–Ω–∞–ª–∏–∑ –≤–∞—à–∏—Ö Telegram-–ø–µ—Ä–µ–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ª–∏—á–Ω–æ—Å—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤.

**üî¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ:** {len(topics.get('topics', []))} –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º  
**üìä –¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞:** 95%+ (–º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)  
**üí∞ –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:** {len([t for t in commercial_assessment.get('commercial_assessment', []) if t.get('commercial_potential') in ['high', 'medium']]) if commercial_assessment else 0}

---
""")
        
        # –¢–æ–ø —Ç–µ–º—ã
        if topics and topics.get('topics'):
            report.append("## üìà –í–ê–®–ò –ì–õ–ê–í–ù–´–ï –ò–ù–¢–ï–†–ï–°–´\n")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É
            sorted_topics = sorted(topics['topics'], key=lambda x: x.get('percentage', 0), reverse=True)
            
            for i, topic in enumerate(sorted_topics[:5], 1):
                sentiment_emoji = {
                    "positive": "üòä –ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è",
                    "neutral": "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è", 
                    "negative": "üòü –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è"
                }.get(topic.get('sentiment', 'neutral'), "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è")
                
                commercial_level = "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
                if commercial_assessment:
                    for comm in commercial_assessment.get('commercial_assessment', []):
                        if comm.get('topic_name') == topic.get('name'):
                            potential = comm.get('commercial_potential', 'low')
                            commercial_level = {
                                'high': 'üî• –í–´–°–û–ö–ò–ô',
                                'medium': '‚≠ê –°–†–ï–î–ù–ò–ô', 
                                'low': 'üí§ –ù–ò–ó–ö–ò–ô'
                            }.get(potential, '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
                            break
                
                report.append(f"""### {i}. {topic['name']} ({topic['percentage']}%)

**üé≠ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {sentiment_emoji}  
**üí∞ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª:** {commercial_level}  
**üîë –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã:** {', '.join(topic['keywords'][:8])}

{topic['description']}

---
""")
        
        # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        if commercial_assessment and commercial_assessment.get('commercial_assessment'):
            report.append("## üí∞ –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –î–õ–Ø –ó–ê–†–ê–ë–û–¢–ö–ê\n")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É
            commercial_topics = commercial_assessment['commercial_assessment']
            high_potential = [t for t in commercial_topics if t.get('commercial_potential') == 'high']
            medium_potential = [t for t in commercial_topics if t.get('commercial_potential') == 'medium']
            
            if high_potential:
                report.append("### üî• –í–´–°–û–ö–ò–ô –ü–û–¢–ï–ù–¶–ò–ê–õ (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –ö –†–ï–ê–õ–ò–ó–ê–¶–ò–ò)\n")
                for topic in high_potential:
                    self._add_commercial_topic_details(report, topic)
            
            if medium_potential:
                report.append("### ‚≠ê –°–†–ï–î–ù–ò–ô –ü–û–¢–ï–ù–¶–ò–ê–õ (–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò)\n")
                for topic in medium_potential:
                    self._add_commercial_topic_details(report, topic)
        
        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report.append("""## üöÄ –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô –ù–ê –ë–õ–ò–ñ–ê–ô–®–ò–ï 30 –î–ù–ï–ô

### ‚úÖ –ü–ï–†–í–´–ï –®–ê–ì–ò (–ù–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ):
""")
        
        if commercial_assessment:
            step_counter = 1
            for topic in commercial_assessment.get('commercial_assessment', []):
                if topic.get('commercial_potential') in ['high', 'medium']:
                    methods = topic.get('monetization_methods', [])
                    if methods and methods[0].get('first_steps'):
                        report.append(f"**{step_counter}. {topic['topic_name']}:**\n")
                        for step in methods[0]['first_steps'][:2]:
                            report.append(f"   - {step}\n")
                        step_counter += 1
                        report.append("\n")
        
        report.append("""### üìä –ü–õ–ê–ù –†–ê–ó–í–ò–¢–ò–Ø (2-4 –Ω–µ–¥–µ–ª–∏):

1. **–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω** –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤
2. **–ó–∞–ø—É—Å—Ç–∏—Ç—å MVP** –æ–¥–Ω–æ–≥–æ –∏–∑ –≤—ã—Å–æ–∫–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π  
3. **–ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏–µ–º–∞ –ø–ª–∞—Ç–µ–∂–µ–π** –∏ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
4. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å** –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∑–Ω–∞–∫–æ–º—ã—Ö

### üéØ –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï (1-3 –º–µ—Å—è—Ü–∞):

1. **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å** —É—Å–ø–µ—à–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
2. **–†–∞—Å—à–∏—Ä–∏—Ç—å** –∞—É–¥–∏—Ç–æ—Ä–∏—é —á–µ—Ä–µ–∑ —Ä–µ–∫–ª–∞–º—É –∏ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞
3. **–î–æ–±–∞–≤–∏—Ç—å** –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã/—É—Å–ª—É–≥–∏
4. **–°–æ–∑–¥–∞—Ç—å** —Å–∏—Å—Ç–µ–º—É –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤

---

## üìû –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò

### ü§ù –•–æ—Ç–∏—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é?
- –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
- –ü–æ–º–æ—â—å –≤ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞  
- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∑–∞–ø—É—Å–∫–∞

### üìä –ù—É–∂–µ–Ω –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑?
- –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –Ω–∏—à–µ
- –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–¥–µ–π

---

**üí° –ü–æ–º–Ω–∏—Ç–µ:** –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≤–∞—à–∏—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ù–∞—á–Ω–∏—Ç–µ —Å —Ç–æ–≥–æ, —á—Ç–æ –≤–∞–º –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –±–ª–∏–∑–∫–æ - —Ç–∞–∫ –±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –Ω–∞ —É—Å–ø–µ—Ö!

*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω TelegramSoul AI System*
""")
        
        return '\n'.join(report)
    
    def _add_commercial_topic_details(self, report: list, topic: dict):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π —Ç–µ–º—ã –≤ –æ—Ç—á–µ—Ç"""
        methods = topic.get('monetization_methods', [])
        if not methods:
            return
            
        main_method = methods[0]
        
        report.append(f"""#### üíº {topic['topic_name']}

**üí∞ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {topic.get('realistic_revenue', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}  
**üéØ –°–ø–æ—Å–æ–± –∑–∞—Ä–∞–±–æ—Ç–∫–∞:** {main_method.get('method', '–ù–µ —É–∫–∞–∑–∞–Ω')}  
**üë• –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {main_method.get('target_audience', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}  
**üí∏ –°—Ç–∞—Ä—Ç–æ–≤—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã:** {main_method.get('startup_cost', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã')}  
**‚è∞ –í—Ä–µ–º—è –¥–æ –ø—Ä–∏–±—ã–ª–∏:** {main_method.get('time_to_profit', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')}  
**üìà –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞:** {main_method.get('success_probability', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')}

**üìù –û–ø–∏—Å–∞–Ω–∏–µ:** {main_method.get('description', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}

**üöÄ –ü–µ—Ä–≤—ã–µ —à–∞–≥–∏:**
""")
        
        for step in main_method.get('first_steps', []):
            report.append(f"- {step}")
        
        report.append(f"\n**üí° –ü–æ—á–µ–º—É –≤–∞–º –ø–æ–¥—Ö–æ–¥–∏—Ç:** {topic.get('why_this_person', '–ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ –¥–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏.')}\n\n---\n")

    async def call_openai_api_with_model(self, messages, model="gpt-4o", temperature=0.3):
        """
        –î–µ–ª–∞–µ—Ç –≤—ã–∑–æ–≤ –∫ OpenAI API —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è API
            model: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            dict: –û—Ç–≤–µ—Ç –æ—Ç API
        """
        try:
            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=self.api_key)
            
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=4000
            )
            
            return {
                'choices': [{
                    'message': {
                        'content': response.choices[0].message.content
                    }
                }]
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API —Å –º–æ–¥–µ–ª—å—é {model}: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
            return await self.call_openai_api(messages, temperature)

    def save_checkpoint(self, chunk_results: List, chunk_index: int, total_chunks: int, filename_base: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            chunk_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π
            chunk_index: –¢–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å —á–∞—Å—Ç–∏
            total_chunks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π  
            filename_base: –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        """
        checkpoint_data = {
            'chunk_results': chunk_results,
            'last_processed_chunk': chunk_index,
            'total_chunks': total_chunks,
            'timestamp': datetime.now().isoformat(),
            'filename_base': filename_base
        }
        
        checkpoint_path = os.path.join(self.output_dir, f"{filename_base}_checkpoint.json")
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: —á–∞—Å—Ç—å {chunk_index}/{total_chunks}")
    
    def load_checkpoint(self, filename_base: str) -> Dict:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç checkpoint –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            filename_base: –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            
        Returns:
            Dict: –î–∞–Ω–Ω—ã–µ checkpoint –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        checkpoint_path = os.path.join(self.output_dir, f"{filename_base}_checkpoint.json")
        
        if os.path.exists(checkpoint_path):
            try:
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                
                logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω checkpoint: –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å —á–∞—Å—Ç–∏ {checkpoint_data.get('last_processed_chunk', 0) + 1}")
                return checkpoint_data
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ checkpoint: {e}")
        
        return None
    
    def cleanup_checkpoint(self, filename_base: str):
        """–£–¥–∞–ª—è–µ—Ç checkpoint –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        checkpoint_path = os.path.join(self.output_dir, f"{filename_base}_checkpoint.json")
        if os.path.exists(checkpoint_path):
            os.remove(checkpoint_path)
            logger.info("üóëÔ∏è Checkpoint —É–¥–∞–ª–µ–Ω –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")

    def generate_beautiful_topic_format(self, topics_data: Dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–º –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        
        Args:
            topics_data: –î–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            
        Returns:
            str: –ö—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        if not topics_data or 'topics' not in topics_data:
            return "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"
        
        topics = topics_data['topics']
        if not topics:
            return "‚ùå –¢–µ–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã - –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ä–∞–∑—É–º–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
        total_raw_percentage = sum(t.get('percentage', 0) for t in topics)
        
        # –ï—Å–ª–∏ —Å—É–º–º–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ 100%, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        if total_raw_percentage > 100:
            normalization_factor = 100 / total_raw_percentage
            for topic in topics:
                topic['normalized_percentage'] = topic.get('percentage', 0) * normalization_factor
        else:
            for topic in topics:
                topic['normalized_percentage'] = topic.get('percentage', 0)
        
        # –†–∞—Å—á–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
        total_periods = 15  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–æ 15 –ø–µ—Ä–∏–æ–¥–∞–º
        
        beautiful_output = []
        beautiful_output.append("üéØ **–ê–ù–ê–õ–ò–ó –í–ê–®–ò–• –ò–ù–¢–ï–†–ï–°–û–í –ò –¢–ï–ú**\n")
        beautiful_output.append("üí° *–ú—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –∏—Å—Ç–æ—Ä–∏—é –≤–∞—à–µ–≥–æ –æ–±—â–µ–Ω–∏—è –Ω–∞ 15 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏, –≤ –∫–∞–∫–∏—Ö –∏–∑ –Ω–∏—Ö –æ–±—Å—É–∂–¥–∞–ª–∞—Å—å –∫–∞–∂–¥–∞—è —Ç–µ–º–∞*\n")
        beautiful_output.append("=" * 50 + "\n")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –ø—Ä–æ—Ü–µ–Ω—Ç—É
        sorted_topics = sorted(topics, key=lambda x: x.get('normalized_percentage', 0), reverse=True)
        
        for i, topic in enumerate(sorted_topics, 1):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
            topic_name = topic.get('name', f'–¢–µ–º–∞ {i}')
            normalized_percentage = topic.get('normalized_percentage', 0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞
            if normalized_percentage >= 20:
                periods_count = max(12, int(total_periods * 0.8))
            elif normalized_percentage >= 15:
                periods_count = max(10, int(total_periods * 0.67))
            elif normalized_percentage >= 10:
                periods_count = max(8, int(total_periods * 0.53))
            elif normalized_percentage >= 5:
                periods_count = max(5, int(total_periods * 0.33))
            else:
                periods_count = max(2, int(total_periods * 0.15))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
            periods_count = min(periods_count, total_periods)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
            if periods_count >= 12:
                status = "üî• –ü–û–°–¢–û–Ø–ù–ù–ê–Ø –¢–ï–ú–ê"
                time_description = "–û–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Å–µ–≥–¥–∞"
                coverage_percent = int((periods_count / total_periods) * 100)
            elif periods_count >= 8:
                status = "‚≠ê –ß–ê–°–¢–ê–Ø –¢–ï–ú–ê"
                time_description = "–û–±—Å—É–∂–¥–∞–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ"
                coverage_percent = int((periods_count / total_periods) * 100)
            elif periods_count >= 5:
                status = "üí° –ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ê–Ø –¢–ï–ú–ê"
                time_description = "–ò–Ω–æ–≥–¥–∞ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è"
                coverage_percent = int((periods_count / total_periods) * 100)
            else:
                status = "üìù –†–ï–î–ö–ê–Ø –¢–ï–ú–ê"
                time_description = "–†–µ–¥–∫–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è"
                coverage_percent = int((periods_count / total_periods) * 100)
            
            # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é —à–∫–∞–ª—É —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
            filled_dots = "‚óè" * periods_count
            empty_dots = "‚óã" * (total_periods - periods_count)
            visual_scale = filled_dots + empty_dots
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–º—É —Å –ø–æ–Ω—è—Ç–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
            beautiful_output.append(f"üî• **{topic_name}**")
            beautiful_output.append(f"üìå {status} - {time_description}")
            beautiful_output.append(f"üìä –ß–∞—Å—Ç–æ—Ç–∞: {visual_scale} (–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ {periods_count} –∏–∑ {total_periods} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç—Ä–µ–∑–∫–æ–≤)")
            beautiful_output.append(f"‚ö° –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {normalized_percentage:.1f}% –æ—Ç –≤—Å–µ—Ö –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            if topic.get('description'):
                beautiful_output.append(f"üí¨ {topic['description']}")
            
            beautiful_output.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–Ω—è—Ç–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        normalized_total = sum(t.get('normalized_percentage', 0) for t in topics)
        beautiful_output.append("üìà **–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê**")
        beautiful_output.append(f"üéØ –ù–∞–π–¥–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è: {len(topics)}")
        beautiful_output.append(f"üìä –û—Ö–≤–∞—Ç –≤–∞—à–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤: {normalized_total:.1f}% —Å–æ–æ–±—â–µ–Ω–∏–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        beautiful_output.append(f"‚è±Ô∏è –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: –≤—Å—è –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ {total_periods} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç—Ä–µ–∑–∫–æ–≤")
        beautiful_output.append("\nüí° **–ö–∞–∫ —á–∏—Ç–∞—Ç—å –æ—Ç—á–µ—Ç:**")
        beautiful_output.append("‚Ä¢ ‚óè = —Ç–µ–º–∞ –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Å—É–∂–¥–∞–ª–∞—Å—å –≤ —ç—Ç–æ–º –ø–µ—Ä–∏–æ–¥–µ")
        beautiful_output.append("‚Ä¢ ‚óã = —Ç–µ–º–∞ –Ω–µ –æ–±—Å—É–∂–¥–∞–ª–∞—Å—å –≤ —ç—Ç–æ–º –ø–µ—Ä–∏–æ–¥–µ")
        beautiful_output.append("‚Ä¢ –ß–µ–º –±–æ–ª—å—à–µ ‚óè, —Ç–µ–º —á–∞—â–µ –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ –Ω–∞ —ç—Ç—É —Ç–µ–º—É")
        
        return "\n".join(beautiful_output)

    def generate_beautiful_soul_report(self, unified_data: Dict, chat_name: str = "–ö–ª–∏–µ–Ω—Ç") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            unified_data: –î–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Ç–µ–º—ã + –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è + –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è)
            chat_name: –ò–º—è –∫–ª–∏–µ–Ω—Ç–∞/—á–∞—Ç–∞
            
        Returns:
            str: –ü–æ–ª–Ω—ã–π –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —Å —Ç—Ä–µ–º—è —É—Ä–æ–≤–Ω—è–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        report_lines = []
        
        # üéØ –ó–ê–ì–û–õ–û–í–û–ö –û–¢–ß–ï–¢–ê
        report_lines.append(f"# üåü SOUL ANALYSIS - –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –†–ê–ó–ë–û–† –¶–ò–§–†–û–í–û–ô –î–£–®–ò")
        report_lines.append(f"## üë§ –ö–ª–∏–µ–Ω—Ç: {chat_name}")
        report_lines.append(f"## üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%d.%m.%Y')}")
        report_lines.append("\n" + "‚ïê" * 70 + "\n")
        
        # üìç –£–†–û–í–ï–ù–¨ 1: –¢–ï–ú–´ –ò –ò–ù–¢–ï–†–ï–°–´
        topics_data = unified_data.get('topics', [])
        if topics_data:
            report_lines.append("## üìç –£–†–û–í–ï–ù–¨ 1: –í–ê–®–ò –û–°–ù–û–í–ù–´–ï –¢–ï–ú–´ –ò –ò–ù–¢–ï–†–ï–°–´")
            report_lines.append("")
            report_lines.append("–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –º—ã –≤—ã—è–≤–∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã:")
            report_lines.append("")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            total_raw_percentage = sum(t.get('percentage', 0) for t in topics_data)
            if total_raw_percentage > 100:
                normalization_factor = 100 / total_raw_percentage
                for topic in topics_data:
                    topic['normalized_percentage'] = topic.get('percentage', 0) * normalization_factor
            else:
                for topic in topics_data:
                    topic['normalized_percentage'] = topic.get('percentage', 0)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É
            sorted_topics = sorted(topics_data, key=lambda x: x.get('normalized_percentage', 0), reverse=True)
            
            for i, topic in enumerate(sorted_topics, 1):
                topic_name = topic.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
                percentage = topic.get('normalized_percentage', 0)
                sentiment = topic.get('sentiment', 'neutral')
                description = topic.get('description', '')
                keywords = topic.get('keywords', [])
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º emoji –∏ —Å—Ç–∞—Ç—É—Å —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º
                if percentage >= 25:
                    emoji = "üî•"
                    status = "–î–û–ú–ò–ù–ò–†–£–Æ–©–ê–Ø –¢–ï–ú–ê"
                elif percentage >= 15:
                    emoji = "‚≠ê"
                    status = "–û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–ï–°"
                elif percentage >= 8:
                    emoji = "üí°"
                    status = "–ó–ê–ú–ï–¢–ù–ê–Ø –¢–ï–ú–ê"
                elif percentage >= 3:
                    emoji = "üìå"
                    status = "–ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ê–Ø –¢–ï–ú–ê"
                else:
                    emoji = "üî∏"
                    status = "–†–ï–î–ö–ê–Ø –¢–ï–ú–ê"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º emoji –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
                if sentiment == 'positive':
                    sentiment_emoji = "üòä"
                    sentiment_text = "–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ"
                elif sentiment == 'negative':
                    sentiment_emoji = "üòî"
                    sentiment_text = "–µ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã"
                else:
                    sentiment_emoji = "üòê"
                    sentiment_text = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ"
                
                # –í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–æ–ª–æ—Å–∞ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                bar_length = int(percentage / 5)  # 5% = 1 —Å–∏–º–≤–æ–ª
                bar_length = min(max(bar_length, 1), 20)  # –û—Ç 1 –¥–æ 20 —Å–∏–º–≤–æ–ª–æ–≤
                intensity_bar = "‚ñà" * bar_length
                
                report_lines.append(f"{emoji} **{i}. {topic_name}** ({percentage:.1f}%)")
                report_lines.append(f"   üìä {intensity_bar}")
                report_lines.append(f"   üè∑Ô∏è {status}")
                report_lines.append(f"   {sentiment_emoji} {sentiment_text}")
                
                if keywords:
                    keywords_text = ", ".join(keywords[:5])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    report_lines.append(f"   üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords_text}")
                
                if description:
                    report_lines.append(f"   üí¨ {description}")
                
                report_lines.append("")
            
            report_lines.append("üìä **–õ–µ–≥–µ–Ω–¥–∞ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏:**")
            report_lines.append("‚ñà = –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª ‚âà 5% –æ—Ç –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            report_lines.append("–ß–µ–º –¥–ª–∏–Ω–Ω–µ–µ –ø–æ–ª–æ—Å–∞, —Ç–µ–º —á–∞—â–µ –≤—ã –æ–±—Å—É–∂–¥–∞–µ—Ç–µ —ç—Ç—É —Ç–µ–º—É")
            report_lines.append("")
        
        # üí∞ –£–†–û–í–ï–ù–¨ 2: –ê–ù–ê–õ–ò–ó –≠–ö–°–ü–ï–†–¢–ù–û–°–¢–ò –ò –ù–ò–®
        expertise_data = unified_data.get('expertise_analysis', [])
        if expertise_data:
            report_lines.append("\n" + "‚ïê" * 70 + "\n")
            report_lines.append("## üí∞ –£–†–û–í–ï–ù–¨ 2: –≠–ö–°–ü–ï–†–¢–ù–û–°–¢–¨ –ò –ù–ò–®–ò")
            report_lines.append("")
            report_lines.append("–ù–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–∏–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –≤–∞—à —É—Ä–æ–≤–µ–Ω—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏:")
            report_lines.append("")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Ä–æ–≤–Ω—é —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
            expertise_order = {'expert': 3, 'advanced': 2, 'beginner': 1}
            sorted_expertise = sorted(
                expertise_data, 
                key=lambda x: expertise_order.get(x.get('expertise_level', 'beginner'), 0), 
                reverse=True
            )
            
            for analysis in sorted_expertise:
                topic_name = analysis.get('topic', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
                expertise_level = analysis.get('expertise_level', 'beginner')
                expertise_indicators = analysis.get('expertise_indicators', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã')
                commercial_potential = analysis.get('commercial_potential', 'low')
                monetization_readiness = analysis.get('monetization_readiness', 'long_term')
                methods = analysis.get('monetization_methods', [])
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º emoji –¥–ª—è —É—Ä–æ–≤–Ω—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
                if expertise_level == 'expert':
                    expertise_emoji = "üèÜ"
                    expertise_text = "–≠–ö–°–ü–ï–†–¢"
                elif expertise_level == 'advanced':
                    expertise_emoji = "üìà"
                    expertise_text = "–ü–†–û–î–í–ò–ù–£–¢–´–ô"
                else:
                    expertise_emoji = "üå±"
                    expertise_text = "–ù–ê–ß–ò–ù–ê–Æ–©–ò–ô"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
                if monetization_readiness == 'ready':
                    readiness_text = "üöÄ –ì–æ—Ç–æ–≤ –∫ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏"
                elif monetization_readiness == 'need_development':
                    readiness_text = "üîß –¢—Ä–µ–±—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—è"
                else:
                    readiness_text = "‚è≥ –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞"
                
                report_lines.append(f"{expertise_emoji} **{topic_name}** - {expertise_text}")
                report_lines.append(f"üí° **–ü—Ä–∏–∑–Ω–∞–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏:** {expertise_indicators}")
                report_lines.append(f"üí∞ **–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –Ω–∏—à–∏:** {commercial_potential}")
                report_lines.append(f"‚ö° **–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏:** {readiness_text}")
                
                if methods:
                    report_lines.append("üõçÔ∏è **–°–ø–æ—Å–æ–±—ã –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏:**")
                    for method in methods[:3]:  # –¢–æ–ø 3 –º–µ—Ç–æ–¥–∞
                        method_name = method.get('method', '–°–ø–æ—Å–æ–± –∑–∞—Ä–∞–±–æ—Ç–∫–∞')
                        time_to_monetization = method.get('time_to_monetization', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        development_needed = method.get('development_needed', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        report_lines.append(f"   ‚Ä¢ **{method_name}**")
                        report_lines.append(f"     ‚è∞ –í—Ä–µ–º—è –¥–æ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏: {time_to_monetization}")
                        report_lines.append(f"     üìö –ù—É–∂–Ω–æ —Ä–∞–∑–≤–∏—Ç—å: {development_needed}")
                
                report_lines.append("")
        
        # üß† –£–†–û–í–ï–ù–¨ 3: –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó
        psychological_data = unified_data.get('psychological_analysis', {})
        if psychological_data:
            report_lines.append("\n" + "‚ïê" * 70 + "\n")
            report_lines.append("## üß† –£–†–û–í–ï–ù–¨ 3: –ì–õ–£–ë–ò–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ –ò –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–Ø")
            report_lines.append("")
            
            system_model = psychological_data.get('system_model', '')
            patterns = psychological_data.get('patterns', [])
            transformation_hint = psychological_data.get('transformation_hint', '')
            
            if system_model:
                report_lines.append("üîÆ **–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è:**")
                report_lines.append(f"{system_model}")
                report_lines.append("")
            
            if patterns:
                report_lines.append("üîç **–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**")
                report_lines.append("")
                
                for i, pattern in enumerate(patterns, 1):
                    name = pattern.get('name', f'–ü–∞—Ç—Ç–µ—Ä–Ω {i}')
                    origin = pattern.get('origin', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    block_effect = pattern.get('block_effect', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')
                    blind_spot = pattern.get('blind_spot', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')
                    related_topics = pattern.get('related_topics', [])
                    
                    report_lines.append(f"**{i}. {name}**")
                    report_lines.append(f"   üå± –ü—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ: {origin}")
                    report_lines.append(f"   üö´ –ö–∞–∫ –±–ª–æ–∫–∏—Ä—É–µ—Ç: {block_effect}")
                    report_lines.append(f"   üëÅÔ∏è –°–ª–µ–ø–∞—è –∑–æ–Ω–∞: {blind_spot}")
                    if related_topics:
                        report_lines.append(f"   üìå –ü—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ —Ç–µ–º–∞—Ö: {', '.join(related_topics)}")
                    report_lines.append("")
            
            if transformation_hint:
                report_lines.append("‚ú® **–ö–õ–Æ–ß –ö –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò:**")
                report_lines.append(f"{transformation_hint}")
                report_lines.append("")
        
        # üéØ –ü–ï–†–°–û–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
        expertise_data = unified_data.get('expertise_analysis', [])
        if expertise_data:
            report_lines.append("\n" + "‚ïê" * 70 + "\n")
            report_lines.append("## üéØ –ü–ï–†–°–û–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
            report_lines.append("")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–º—É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É –ò –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            priority_topics = []
            for analysis in expertise_data:
                topic_name = analysis.get('topic', '')
                commercial_potential = analysis.get('commercial_potential', 'low')
                monetization_readiness = analysis.get('monetization_readiness', 'long_term')
                expertise_level = analysis.get('expertise_level', 'beginner')
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (0-10)
                priority_score = 0
                
                # –ë–∞–ª–ª—ã –∑–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
                if commercial_potential == 'high':
                    priority_score += 4
                elif commercial_potential == 'medium':
                    priority_score += 2
                
                # –ë–∞–ª–ª—ã –∑–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
                if monetization_readiness == 'ready':
                    priority_score += 3
                elif monetization_readiness == 'need_development':
                    priority_score += 1
                
                # –ë–∞–ª–ª—ã –∑–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å
                if expertise_level == 'expert':
                    priority_score += 3
                elif expertise_level == 'advanced':
                    priority_score += 2
                else:
                    priority_score += 1
                
                priority_topics.append({
                    'topic': topic_name,
                    'analysis': analysis,
                    'priority': priority_score
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            priority_topics.sort(key=lambda x: x['priority'], reverse=True)
            
            report_lines.append("–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º:")
            report_lines.append("")
            
            for i, item in enumerate(priority_topics[:3], 1):
                analysis = item['analysis']
                topic_name = analysis.get('topic', f'–¢–µ–º–∞ {i}')
                commercial_potential = analysis.get('commercial_potential', 'low')
                monetization_readiness = analysis.get('monetization_readiness', 'long_term')
                expertise_level = analysis.get('expertise_level', 'beginner')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ª–æ–≥–∏—á–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                if expertise_level == 'expert' and commercial_potential == 'high' and monetization_readiness == 'ready':
                    rec = "üöÄ –ù–ê–ß–ò–ù–ê–ô–¢–ï –ú–û–ù–ï–¢–ò–ó–ê–¶–ò–Æ –ù–ï–ú–ï–î–õ–ï–ù–ù–û! –£ –≤–∞—Å –µ—Å—Ç—å –≤—Å–µ –¥–ª—è —É—Å–ø–µ—Ö–∞"
                elif expertise_level in ['expert', 'advanced'] and commercial_potential in ['high', 'medium']:
                    rec = "üí™ –†–∞–∑–≤–∏–≤–∞–π—Ç–µ –∞–∫—Ç–∏–≤–Ω–æ - —É –≤–∞—Å –æ—Ç–ª–∏—á–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"
                elif commercial_potential == 'high' and expertise_level == 'beginner':
                    rec = "üìö –°–Ω–∞—á–∞–ª–∞ —É–≥–ª—É–±–∏—Ç–µ –∑–Ω–∞–Ω–∏—è, –ø–æ—Ç–æ–º –º–æ–Ω–µ—Ç–∏–∑–∏—Ä—É–π—Ç–µ"
                elif expertise_level in ['expert', 'advanced'] and commercial_potential == 'low':
                    rec = "üéØ –†–∞–∑–≤–∏–≤–∞–π—Ç–µ –¥–ª—è –¥—É—à–∏, –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç"
                else:
                    rec = "üå± –†–∞–∑–≤–∏–≤–∞–π—Ç–µ—Å—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –æ—Ü–µ–Ω–∏–≤–∞–π—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"
                
                report_lines.append(f"**{i}. {topic_name}**")
                report_lines.append(f"   üéØ –£—Ä–æ–≤–µ–Ω—å: {expertise_level} | –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª: {commercial_potential} | –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {monetization_readiness}")
                report_lines.append(f"   üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {rec}")
                report_lines.append("")
        
        # –ü–û–î–ü–ò–°–¨
        report_lines.append("‚îÄ" * 70)
        report_lines.append("üåü *Soul Analysis Report - –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ü–∏—Ñ—Ä–æ–≤–æ–π –¥—É—à–∏*")
        report_lines.append("üìä *–°–∏—Å—Ç–µ–º–∞ TelegramSoul –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ò–ò –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞*")
        report_lines.append(f"‚è∞ *–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {datetime.now().strftime('%d.%m.%Y %H:%M')}*")
        
        return "\n".join(report_lines)

# üöÄ –≠–¢–ê–ü 1: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –¥–ª—è –≤—Å–µ—Ö —á–∞—Ç–æ–≤
FAST_TOPIC_ANALYSIS_PROMPT = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –±—ã—Å—Ç—Ä–æ –∏ —Ç–æ—á–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏–∑ Telegram-–ø–µ—Ä–µ–ø–∏—Å–∫–∏.

üìç –ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –≤—ã–¥–µ–ª–∏ 10-15 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.

üìã –ß–¢–û –ù–£–ñ–ù–û –°–î–ï–õ–ê–¢–¨:
1. –í—ã–¥–µ–ª–∏ 10-15 —Ç–µ–º (–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫ –º–µ–Ω–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º)
2. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã —É–∫–∞–∂–∏:
   - –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã (–∫—Ä–∞—Ç–∫–æ–µ –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ)
   - –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (5-8 —Å–ª–æ–≤ –º–∞–∫—Å–∏–º—É–º)
   - –ü—Ä–æ—Ü–µ–Ω—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π (–í–ê–ñ–ù–û: —Å—É–º–º–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ = 100%)
   - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (positive/negative/neutral)
   - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

‚ö° –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ë—É–¥—å —Ç–æ—á–Ω—ã–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö ‚Äî –∏—Ö —Å—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–æ–≤–Ω–æ 100%
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ö, –∏–≥–Ω–æ—Ä–∏—Ä—É–π –±—ã—Ç–æ–≤—ã–µ –º–µ–ª–æ—á–∏
- –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–º—ã –≤–º–µ—Å—Ç–µ
- –ù–∞–∑—ã–≤–∞–π —Ç–µ–º—ã –ø–æ–Ω—è—Ç–Ω–æ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞

–°–û–û–ë–©–ï–ù–ò–Ø –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{messages}

üì§ –í–ï–†–ù–ò –†–ï–ó–£–õ–¨–¢–ê–¢ –í JSON:
{{
  "topics": [
    {{
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
      "keywords": ["—Å–ª–æ–≤–æ1", "—Å–ª–æ–≤–æ2", "—Å–ª–æ–≤–æ3"],
      "percentage": XX.X,
      "sentiment": "positive/negative/neutral",
      "description": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–º—ã"
    }},
    ...
  ]
}}
"""

# üéØ –≠–¢–ê–ü 2: –ì–ª—É–±–æ–∫–∏–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ç–µ–º
FINAL_ANALYSIS_PROMPT = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —Å—Ç—Ä–∞—Ç–µ–≥ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ—É—á. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö —Ç–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.

üìä –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
{aggregated_topics}

üéØ –ó–ê–î–ê–ß–ê: –ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑:

---

üí∞ –£–†–û–í–ï–ù–¨ 1: –ê–ù–ê–õ–ò–ó –≠–ö–°–ü–ï–†–¢–ù–û–°–¢–ò –ò –ù–ò–®
–î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π:
1. –ì–ª—É–±–∏–Ω—É —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (expert/advanced/beginner)
2. –û—Ü–µ–Ω–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –Ω–∏—à–∏ (high/medium/low)
3. –ü—Ä–µ–¥–ª–æ–∂–∏ 2-3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–∞ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
4. –û–ø—Ä–µ–¥–µ–ª–∏ —Å—Ä–æ–∫ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–æ —É—Ä–æ–≤–Ω—è –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞

üß† –£–†–û–í–ï–ù–¨ 2: –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ü–ê–¢–¢–ï–†–ù–´ –ò –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–Ø
–ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–π —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ —Ç–µ–º –∏ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤:
1. –û–ø—Ä–µ–¥–µ–ª–∏ –∞—Ä—Ö–µ—Ç–∏–ø –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–º–µ—Ç–∞—Ñ–æ—Ä–∞ –ª–∏—á–Ω–æ—Å—Ç–∏)
2. –í—ã—è–≤–∏ 3-5 —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Ä–æ—Å—Ç
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —É–∫–∞–∂–∏:
   - –ü—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (—Å–µ–º—å—è/–∫—É–ª—å—Ç—É—Ä–∞/—Ç—Ä–∞–≤–º–∞/—Å–æ—Ü–∏—É–º)
   - –í –∫–∞–∫–∏—Ö —Ç–µ–º–∞—Ö –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è
   - –ö–∞–∫ –±–ª–æ–∫–∏—Ä—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ
   - –ö–∞–∫—É—é —Å–ª–µ–ø—É—é –∑–æ–Ω—É —Å–æ–∑–¥–∞–µ—Ç
4. –î–∞–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π

üì§ –í–ï–†–ù–ò –†–ï–ó–£–õ–¨–¢–ê–¢ –í JSON:
{{
  "expertise_analysis": [
    {{
      "topic": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
      "expertise_level": "expert/advanced/beginner",
      "expertise_indicators": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π",
      "commercial_potential": "high/medium/low",
      "monetization_readiness": "ready/need_development/long_term",
      "monetization_methods": [
        {{
          "method": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–∞",
          "description": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",
          "time_to_monetization": "2-4 –º–µ—Å—è—Ü–∞",
          "development_needed": "–ß—Ç–æ –Ω—É–∂–Ω–æ —Ä–∞–∑–≤–∏—Ç—å –¥–ª—è –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏"
        }}
      ]
    }}
  ],
  "psychological_analysis": {{
    "system_model": "–ú–µ—Ç–∞—Ñ–æ—Ä–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–µ—Ç–∏–ø–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –∫—Ç–æ –æ–Ω –ø–æ —Å—É—Ç–∏",
    "patterns": [
      {{
        "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞",
        "origin": "–í–æ–∑–º–æ–∂–Ω–æ–µ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (—Å–µ–º—å—è/–∫—É–ª—å—Ç—É—Ä–∞/—Ç—Ä–∞–≤–º–∞/—Å–æ—Ü–∏—É–º)",
        "related_topics": ["—Ç–µ–º–∞1", "—Ç–µ–º–∞2"],
        "block_effect": "–ö–∞–∫ –∏–º–µ–Ω–Ω–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ —Ä–æ—Å—Ç",
        "blind_spot": "–ö–∞–∫—É—é —Å–ª–µ–ø—É—é –∑–æ–Ω—É —Å–æ–∑–¥–∞–µ—Ç"
      }}
    ],
    "transformation_hint": "–ö–ª—é—á–µ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π"
  }}
}}
"""
