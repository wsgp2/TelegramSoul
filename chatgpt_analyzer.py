#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ChatGPT Telegram Chat Analyzer

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API (gpt-4o-mini) –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram —á–∞—Ç–æ–≤,
–≤—ã—è–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º, —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π.

–û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
"""

import os
import json
import logging
import asyncio
import time
from datetime import datetime
import pandas as pd
import numpy as np
import httpx
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
import difflib
from wordcloud import WordCloud
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', f'chatgpt_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'))
    ]
)

logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MESSAGES_DIR = os.path.join(BASE_DIR, 'data', 'messages')
OUTPUT_DIR = os.path.join(BASE_DIR, 'data', 'reports')
VISUALIZATION_DIR = os.path.join(OUTPUT_DIR, 'visualizations')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
for directory in [MESSAGES_DIR, OUTPUT_DIR, VISUALIZATION_DIR, LOGS_DIR]:
    os.makedirs(directory, exist_ok=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI API
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')
MAX_TOKENS = 32000  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è gpt-4o-mini


class ChatGPTAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Telegram-—á–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT (gpt-4o-mini)
    """
    
    def __init__(self, api_key=None, model="gpt-4o-mini"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            api_key (str): API –∫–ª—é—á OpenAI. –ï—Å–ª–∏ None, –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OPENAI_API_KEY –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            model (str): –ú–æ–¥–µ–ª—å OpenAI –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        self.api_key = api_key or OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("API –∫–ª—é—á OpenAI –Ω–µ —É–∫–∞–∑–∞–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –∫–ª—é—á –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞")
        
        self.model = model
        self.messages_dir = MESSAGES_DIR
        self.output_dir = OUTPUT_DIR
        self.visualization_dir = VISUALIZATION_DIR
        self.client = httpx.AsyncClient(timeout=60.0)
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ChatGPT-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –º–æ–¥–µ–ª—å—é {model}")
        
    async def load_messages_from_dir(self, directory=None) -> List[Dict]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ JSON —Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        
        Args:
            directory (str, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ —Å–æ–æ–±—â–µ–Ω–∏–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é self.messages_dir
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        directory = directory or self.messages_dir
        all_messages = []
        user_ids = []
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–ø–∞–ø–æ–∫) –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        try:
            user_ids = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]
        except FileNotFoundError:
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []
            
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        for user_id in tqdm(user_ids, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —á–∞—Ç–æ–≤"):
            messages_file = os.path.join(directory, user_id, "messages.json")
            try:
                with open(messages_file, 'r', encoding='utf-8') as f:
                    user_messages = json.load(f)
                    all_messages.extend(user_messages)
            except FileNotFoundError:
                logger.warning(f"–§–∞–π–ª —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            except json.JSONDecodeError:
                logger.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π JSON —Ñ–æ—Ä–º–∞—Ç –≤ —Ñ–∞–π–ª–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        return all_messages
    
    def prepare_messages_for_analysis(self, messages: List[Dict], sample_size=None) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –≤—ã–±–∏—Ä–∞—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —É–¥–∞–ª—è—è —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
        Args:
            messages (List[Dict]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            sample_size (int, optional): –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            
        Returns:
            List[str]: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        text_messages = []
        for msg in messages:
            if 'content' in msg and isinstance(msg['content'], str) and len(msg['content'].strip()) > 10:
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                content = msg['content']
                if not content.startswith('/') and not content.startswith('@'):
                    text_messages.append(content)
            elif 'text' in msg and isinstance(msg['text'], str) and len(msg['text'].strip()) > 10:
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                text = msg['text']
                if not text.startswith('/') and not text.startswith('@'):
                    text_messages.append(text)
            elif 'message' in msg and isinstance(msg['message'], str) and len(msg['message'].strip()) > 2:  
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞–º
                message = msg['message']
                if not message.startswith('/') and not message.startswith('@'):
                    text_messages.append(message)
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω sample_size, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
        if sample_size and len(text_messages) > sample_size:
            np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            indices = np.random.choice(len(text_messages), sample_size, replace=False)
            text_messages = [text_messages[i] for i in indices]
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(text_messages)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return text_messages
    
    async def call_openai_api(self, messages, temperature=0.3):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç OpenAI API —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        
        Args:
            messages (List[Dict]): –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è API –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"role": "...", "content": "..."}]
            temperature (float): –ü–∞—Ä–∞–º–µ—Ç—Ä temperature –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            Dict: –û—Ç–≤–µ—Ç –æ—Ç API
        """
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4000
        }
        
        try:
            response = await self.client.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
            raise
            
    async def analyze_topics(self, text_messages: List[str], max_tokens_per_chunk: int = 8000):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—ã –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT
        
        Args:
            text_messages (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            max_tokens_per_chunk (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ API
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö...")
        
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –º–∞–ª–æ, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å—Ä–∞–∑—É
        if len('\n'.join(text_messages)) < 10000:  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
            messages_text = '\n'.join(text_messages)
            prompt = TOPIC_ANALYSIS_PROMPT.format(messages=messages_text)
            
            messages_for_api = [
                {"role": "system", "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –≤—ã—è–≤–ª–µ–Ω–∏—é —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö."},
                {"role": "user", "content": prompt}
            ]
            
            try:
                response = await self.call_openai_api(messages_for_api, temperature=0.2)
                content = response['choices'][0]['message']['content']
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
                return self.extract_json_from_text(content)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–º: {e}")
                return {"topics": []}
        
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –º–Ω–æ–≥–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        chunk_size = len(text_messages) // ((len('\n'.join(text_messages)) // max_tokens_per_chunk) + 1)
        chunk_messages = ['\n'.join(text_messages[i:i + chunk_size]) for i in range(0, len(text_messages), chunk_size)]
        
        logger.info(f"–°–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(chunk_messages)} —á–∞—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        all_topics = []
        for i, chunk in enumerate(chunk_messages):
            logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å {i+1} –∏–∑ {len(chunk_messages)}")
            messages = [
                {"role": "system", "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –≤—ã—è–≤–ª–µ–Ω–∏—é —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö."},
                {"role": "user", "content": TOPIC_ANALYSIS_PROMPT.format(messages=chunk)}
            ]
            
            try:
                response = await self.call_openai_api(messages, temperature=0.2)
                content = response['choices'][0]['message']['content']
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
                chunk_topics = self.extract_json_from_text(content)
                all_topics.extend(chunk_topics.get("topics", []))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —á–∞—Å—Ç–∏ {i+1}: {e}")
                continue
                
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã
        aggregated_topics = self._aggregate_similar_topics(all_topics)
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω. –í—ã—è–≤–ª–µ–Ω–æ {len(aggregated_topics)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º")
        return {"topics": aggregated_topics}
    
    def extract_json_from_text(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        import re  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º re –≤–Ω—É—Ç—Ä–∏ –º–µ—Ç–æ–¥–∞
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        os.makedirs(LOGS_DIR, exist_ok=True)
        log_file_path = os.path.join(LOGS_DIR, f"json_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(f"--- –¢–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON ---\n{text}\n--- –ö–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞ ---")
        
        logger.info(f"–¢–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {log_file_path}")
        logger.info(f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON:\n{text[:500]}")
        logger.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON:\n{text[-500:] if len(text) > 500 else text}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 0: –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–±–µ—Ä–Ω—É—Ç –≤ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫ –∫–æ–¥–∞ ```json ... ```
            code_block_pattern = r'```(?:json)?(.*?)```'
            code_blocks = re.findall(code_block_pattern, text, re.DOTALL)
            
            if code_blocks:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –±–ª–æ–∫ –∫–æ–¥–∞
                json_text = code_blocks[0].strip()
                logger.info(f"–ù–∞–π–¥–µ–Ω JSON –≤ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–µ –∫–æ–¥–∞, –¥–ª–∏–Ω–∞: {len(json_text)}")
                try:
                    return json.loads(json_text)
                except json.JSONDecodeError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –∏–∑ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–∞: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–∞—Ä–∫–¥–∞—É–Ω –±–ª–æ–∫–∞: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}, –ø–æ–∑–∏—Ü–∏—è: {e.pos}, —Å—Ç—Ä–æ–∫–∞: {text[max(0, e.pos-50):e.pos+50] if e.pos < len(text) else '–∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞'}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
        try:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏
            open_braces = [m.start() for m in re.finditer('{', text)]
            close_braces = [m.start() for m in re.finditer('}', text)]
            
            if open_braces and close_braces:
                # –ù–∞–π–¥–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–∞—Ä—É —Å–∫–æ–±–æ–∫, —É—á–∏—Ç—ã–≤–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                start_index = open_braces[0]  # –ü–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
                
                # –°—á–∏—Ç–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å —Å–∫–æ–±–æ–∫
                depth = 0
                for i in range(len(text)):
                    if i in open_braces:
                        depth += 1
                    elif i in close_braces:
                        depth -= 1
                        if depth == 0 and i > start_index:
                            # –ù–∞—à–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
                            end_index = i
                            json_str = text[start_index:end_index+1]
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏: {start_index} - {end_index}")
                            return json.loads(json_str)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–∫–æ–±–æ–∫ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–π –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ñ–∏–≥—É—Ä–Ω–æ–π —Å–∫–æ–±–∫–∏
        try:
            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é —Ñ–∏–≥—É—Ä–Ω—É—é —Å–∫–æ–±–∫—É
            start_index = text.find('{')
            end_index = text.rfind('}')
            
            if start_index >= 0 and end_index > start_index:
                json_str = text[start_index:end_index+1]
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: –Ω–∞—á–∞–ª–æ {start_index}, –∫–æ–Ω–µ—Ü {end_index}")
                logger.info(f"–ò–∑–≤–ª–µ–∫–∞–µ–º—ã–π JSON: {json_str[:100]}...{json_str[-100:] if len(json_str) > 200 else json_str}")
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: {e}")
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–±–æ–∫: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON –æ–±—ä–µ–∫—Ç–∞
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            json_pattern = r'\{[\s\S]*?\}'
            matches = list(re.finditer(json_pattern, text))
            
            if matches:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
                candidate = None
                max_length = 0
                
                for match in matches:
                    json_str = match.group(0)
                    if len(json_str) > max_length:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON
                            json.loads(json_str)
                            candidate = json_str
                            max_length = len(json_str)
                        except:
                            continue
                
                if candidate:
                    logger.info(f"–ù–∞–π–¥–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è")
                    return json.loads(candidate)
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ JSON —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: {e}")
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: {e}")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ –≤ JSON
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ
            fixed_text = text.replace("'", '"')
            return json.loads(fixed_text)
        except json.JSONDecodeError:
            pass
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 6: –ü–æ–∏—Å–∫ —Å—Ç—Ä–æ–∫–∏ "topics": –∏ –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ–±—ä–µ–∫—Ç–∞
        try:
            topics_index = text.find('"topics":')
            if topics_index > 0:
                # –ò—â–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –ø–µ—Ä–µ–¥ "topics"
                open_brace_index = text.rfind('{', 0, topics_index)
                if open_brace_index >= 0:
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                    json_str = text[open_brace_index:]
                    depth = 0
                    for i, char in enumerate(json_str):
                        if char == '{':
                            depth += 1
                        elif char == '}':
                            depth -= 1
                            if depth == 0:
                                json_str = json_str[:i+1]
                                logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω JSON –æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º 'topics': {json_str[:100]}...{json_str[-100:] if len(json_str) > 200 else json_str}")
                                return json.loads(json_str)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –ø–æ –∫–ª—é—á—É 'topics': {e}")
        
        # –ï—Å–ª–∏ –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —à–∞–±–ª–æ–Ω")
        return {"topics": [{
            "name": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
            "keywords": ["–æ—à–∏–±–∫–∞"],
            "percentage": 100,
            "sentiment": "neutral",
            "description": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—Ç–≤–µ—Ç–∞ API."
        }]}
    
    def _aggregate_similar_topics(self, topics: List[Dict]) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        Args:
            topics (List[Dict]): –°–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º
        """
        if not topics:
            return []
            
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å —Å–∞–º—ã—Ö –∑–Ω–∞—á–∏–º—ã—Ö
        sorted_topics = sorted(topics, key=lambda x: x.get('percentage', 0), reverse=True)
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —Ç–µ–º
        processed_topics = {}
        
        for topic in sorted_topics:
            topic_name = topic.get('name', '').lower()
            keywords = set([k.lower() for k in topic.get('keywords', [])])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –ø–æ—Ö–æ–∂–∞—è —Ç–µ–º–∞
            found_match = False
            for key, existing_topic in processed_topics.items():
                existing_name = existing_topic.get('name', '').lower()
                existing_keywords = set([k.lower() for k in existing_topic.get('keywords', [])])
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ—Ö–æ–∂–∏
                keyword_similarity = len(keywords.intersection(existing_keywords)) / max(len(keywords), len(existing_keywords)) if keywords and existing_keywords else 0
                name_similarity = difflib.SequenceMatcher(None, topic_name, existing_name).ratio()
                
                if keyword_similarity > 0.3 or name_similarity > 0.7:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–º—ã
                    new_percentage = (existing_topic.get('percentage', 0) + topic.get('percentage', 0))
                    existing_topic['percentage'] = new_percentage
                    
                    # –†–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    unique_keywords = list(set(existing_topic.get('keywords', []) + topic.get('keywords', [])))
                    existing_topic['keywords'] = unique_keywords[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    
                    found_match = True
                    break
            
            if not found_match:
                processed_topics[topic_name] = topic
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–∞
        result = list(processed_topics.values())
        result = sorted(result, key=lambda x: x.get('percentage', 0), reverse=True)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ 100%
        total_percentage = sum(topic.get('percentage', 0) for topic in result)
        if total_percentage > 0:
            for topic in result:
                topic['percentage'] = round((topic.get('percentage', 0) / total_percentage) * 100, 1)
        
        return result[:7]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-7 —Ç–µ–º
        
    async def develop_monetization_strategies(self, topics: Dict):
        """
        –†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–º
        
        Args:
            topics (Dict): JSON –æ–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏...")
        
        if not topics or not topics.get('topics'):
            logger.warning("–ù–µ—Ç —Ç–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏")
            return {"monetization_strategies": []}
            
        topics_json = json.dumps(topics, ensure_ascii=False, indent=2)
        prompt = MONETIZATION_ANALYSIS_PROMPT.format(topics_json=topics_json)
        
        messages_for_api = [
            {"role": "system", "content": "–í—ã - –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª–µ–π."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = await self.call_openai_api(messages_for_api, temperature=0.4)
            content = response['choices'][0]['message']['content']
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º extract_json_from_text –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ json.loads
            result = self.extract_json_from_text(content)
            
            logger.info(f"–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ {len(result.get('monetization_strategies', []))} —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏")
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏: {e}")
            return {"monetization_strategies": []}
            
    async def create_business_plan(self, topics: Dict, monetization: Dict):
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            topics (Dict): JSON –æ–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            monetization (Dict): JSON –æ–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            
        Returns:
            Dict: –î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞...")
        
        if not topics or not monetization:
            logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞")
            return {"business_plan": {}}
            
        topics_json = json.dumps(topics, ensure_ascii=False, indent=2)
        monetization_json = json.dumps(monetization, ensure_ascii=False, indent=2)
        
        prompt = BUSINESS_PLAN_PROMPT.format(
            topics_json=topics_json,
            monetization_json=monetization_json
        )
        
        messages_for_api = [
            {"role": "system", "content": "–í—ã - –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-—Å—Ç—Ä–∞—Ç–µ–≥ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –∑–∞–ø—É—Å–∫—É —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ —Å –æ–±—à–∏—Ä–Ω—ã–º –æ–ø—ã—Ç–æ–º —Å–æ–∑–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–æ–≤."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = await self.call_openai_api(messages_for_api, temperature=0.3)
            content = response['choices'][0]['message']['content']
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º extract_json_from_text –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ json.loads
            result = self.extract_json_from_text(content)
            
            logger.info("–ë–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞: {e}")
            return {"business_plan": {}}
        
    def save_results_to_json(self, data: Dict, filename: str, directory: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª
        
        Args:
            data (Dict): –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filename (str): –ò–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
            directory (str, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é self.output_dir
            
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        directory = directory or self.output_dir
        os.makedirs(directory, exist_ok=True)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = os.path.join(directory, f"{filename}_{timestamp}.json")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        return filepath
        
    def generate_report(self, topics: Dict, monetization: Dict = None, business_plan: Dict = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            topics (Dict): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            monetization (Dict, optional): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            business_plan (Dict, optional): –î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω
            
        Returns:
            str: –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
        """
        report = ["# –û—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ Telegram-—á–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT\n"]
        report.append(f"*–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å —Ç–µ–º–∞–º–∏
        if topics and topics.get('topics'):
            report.append("## 1. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è\n")
            for i, topic in enumerate(topics['topics'], 1):
                sentiment_emoji = {
                    "positive": "üòä",
                    "neutral": "üòê",
                    "negative": "üòü"
                }.get(topic.get('sentiment', 'neutral'), "üòê")
                
                report.append(f"### {i}. {topic['name']} ({topic['percentage']}%) {sentiment_emoji}\n")
                report.append(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(topic['keywords'])}\n")
                report.append(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {topic['description']}\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
        if monetization and monetization.get('monetization_strategies'):
            report.append("## 2. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏\n")
            for i, strategy in enumerate(monetization['monetization_strategies'], 1):
                report.append(f"### {i}. –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–º—ã '{strategy['topic']}'\n")
                
                for j, product in enumerate(strategy['products'], 1):
                    revenue_emoji = {
                        "high": "üí∞üí∞üí∞",
                        "medium": "üí∞üí∞",
                        "low": "üí∞"
                    }.get(product.get('revenue_potential', '').lower(), "üí∞")
                    
                    complexity_emoji = {
                        "high": "üî¥",
                        "medium": "üü†",
                        "low": "üü¢"
                    }.get(product.get('implementation_complexity', '').lower(), "üü†")
                    
                    report.append(f"#### {j}. {product['name']} {revenue_emoji} {complexity_emoji}\n")
                    report.append(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {product['description']}\n")
                    report.append(f"**–ú–æ–¥–µ–ª—å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏:** {product['model']}\n")
                    report.append(f"**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {product['revenue_potential']}\n")
                    report.append(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:** {product['implementation_complexity']}\n")
                    report.append(f"**–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏:** {product['timeframe']}\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–æ–º
        if business_plan and business_plan.get('business_plan'):
            bp = business_plan['business_plan']
            report.append("## 3. –î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω\n")
            
            if bp.get('executive_summary'):
                report.append("### 3.1. –†–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞\n")
                report.append(f"**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:** {bp['executive_summary'].get('concept', '')}\n")
                report.append(f"**–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {bp['executive_summary'].get('target_audience', '')}\n")
                report.append(f"**–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:** {bp['executive_summary'].get('value_proposition', '')}\n")
            
            if bp.get('market_analysis'):
                report.append("### 3.2. –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞\n")
                report.append(f"**–†–∞–∑–º–µ—Ä —Ä—ã–Ω–∫–∞:** {bp['market_analysis'].get('market_size', '')}\n")
                
                if bp['market_analysis'].get('trends'):
                    report.append("**–¢–µ–Ω–¥–µ–Ω—Ü–∏–∏:**\n")
                    for trend in bp['market_analysis']['trends']:
                        report.append(f"- {trend}\n")
                
                if bp['market_analysis'].get('competitors'):
                    report.append("**–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã:**\n")
                    for competitor in bp['market_analysis']['competitors']:
                        report.append(f"- {competitor}\n")
            
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞ –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏...
            
        report.append("\n---\n")
        report.append("*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT Analyzer*")
        
        return '\n'.join(report)
    
    def visualize_topics(self, topics: Dict, output_dir: str = None) -> List[str]:
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
        
        Args:
            topics (Dict): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
            output_dir (str, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º
        """
        output_dir = output_dir or self.visualization_dir
        os.makedirs(output_dir, exist_ok=True)
        saved_files = []
        
        if not topics or not topics.get('topics'):
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ç–µ–º")
            return saved_files
        
        topic_data = topics.get('topics', [])
        
        try:
            # 1. –°–æ–∑–¥–∞–µ–º –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
            plt.figure(figsize=(12, 8))
            labels = [topic['name'] for topic in topic_data]
            sizes = [topic['percentage'] for topic in topic_data]
            colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—Ç—É–ø –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–∞–º–æ–π –±–æ–ª—å—à–æ–π —Ç–µ–º—ã
            explode = [0.1 if i == np.argmax(sizes) else 0 for i in range(len(sizes))]
            
            plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
                    shadow=True, startangle=90)
            plt.axis('equal')  # –î–µ–ª–∞–µ–º –∫—Ä—É–≥ —Ä–∞–≤–Ω—ã–º
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –≤ —á–∞—Ç–µ', fontsize=16, pad=20)
            
            pie_chart_path = os.path.join(output_dir, f"topics_distribution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            plt.savefig(pie_chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            saved_files.append(pie_chart_path)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º: {pie_chart_path}")
            
            # 2. –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            all_keywords = []
            keyword_weights = {}
            
            for topic in topic_data:
                for keyword in topic['keywords']:
                    if keyword in keyword_weights:
                        keyword_weights[keyword] += topic['percentage']
                    else:
                        keyword_weights[keyword] = topic['percentage']
                    all_keywords.append(keyword)
            
            if all_keywords:
                plt.figure(figsize=(14, 10))
                wordcloud = WordCloud(
                    width=1000, height=600,
                    background_color='white',
                    max_words=100,
                    colormap='viridis',
                    collocations=False
                ).generate_from_frequencies(keyword_weights)
                
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis("off")
                plt.title('–û–±–ª–∞–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤—Å–µ—Ö —Ç–µ–º', fontsize=16)
                
                wordcloud_path = os.path.join(output_dir, f"keywords_wordcloud_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
                plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')
                plt.close()
                saved_files.append(wordcloud_path)
                logger.info(f"–°–æ–∑–¥–∞–Ω–æ –æ–±–ª–∞–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {wordcloud_path}")
            
            # 3. –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–≤
            sentiment_counts = Counter([topic.get('sentiment', 'neutral') for topic in topic_data])
            sentiment_df = pd.DataFrame({
                '–°–µ–Ω—Ç–∏–º–µ–Ω—Ç': list(sentiment_counts.keys()),
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º': list(sentiment_counts.values())
            })
            
            plt.figure(figsize=(10, 6))
            sns.barplot(x='–°–µ–Ω—Ç–∏–º–µ–Ω—Ç', y='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º', data=sentiment_df, palette=['green', 'gray', 'red'])
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º', fontsize=16)
            plt.xlabel('–°–µ–Ω—Ç–∏–º–µ–Ω—Ç', fontsize=12)
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º', fontsize=12)
            
            sentiment_path = os.path.join(output_dir, f"sentiment_distribution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            plt.savefig(sentiment_path, dpi=300, bbox_inches='tight')
            plt.close()
            saved_files.append(sentiment_path)
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–æ–≤: {sentiment_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")
        
        return saved_files
        
    async def run_full_analysis(self, chat_name: str, messages_limit: int = None, save_results: bool = True):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —á–∞—Ç–∞
        
        Args:
            chat_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            messages_limit (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            save_results (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞—Ç–∞ '{chat_name}'")
        results = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = await self.load_messages_from_dir(directory=os.path.join(self.messages_dir, chat_name))
        if not messages:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —á–∞—Ç–∞ '{chat_name}'")
            return results
            
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —á–∞—Ç–∞ '{chat_name}'")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–º—ã
        topics_result = await self.analyze_topics(self.prepare_messages_for_analysis(messages, sample_size=messages_limit))
        if not topics_result or not topics_result.get('topics'):
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—ã")
            return results
            
        results['topic_analysis'] = topics_result
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ç–µ–º
        if save_results:
            self.save_results_to_json(topics_result, f"{chat_name}_topics_analysis")
            
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
        monetization_result = await self.develop_monetization_strategies(topics_result)
        if monetization_result and monetization_result.get('monetization_strategies'):
            results['monetization_analysis'] = monetization_result
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
            if save_results:
                self.save_results_to_json(monetization_result, f"{chat_name}_monetization_strategies")
                
        # –°–æ–∑–¥–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω
        if topics_result and monetization_result:
            business_plan_result = await self.create_business_plan(topics_result, monetization_result)
            if business_plan_result and business_plan_result.get('business_plan'):
                results['business_plan'] = business_plan_result
                
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω
                if save_results:
                    self.save_results_to_json(business_plan_result, f"{chat_name}_business_plan")
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if topics_result:
            visualizations = self.visualize_topics(topics_result)
            results['visualizations'] = visualizations
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if save_results and (topics_result or monetization_result or results.get('business_plan')):
            report_text = self.generate_report(
                topics_result,
                results.get('monetization_analysis'),
                results.get('business_plan')
            )
            
            report_path = os.path.join(self.output_dir, f"{chat_name}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
                
            logger.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")
            results['report_path'] = report_path
        
        logger.info(f"–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞—Ç–∞ '{chat_name}' –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        return results

# –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º
# –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º–∞—Ç–∏–∫–∏
TOPIC_ANALYSIS_PROMPT = """
–ö–∞–∫ –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Telegram-—á–∞—Ç–∞. 
–í—ã–≤–µ–¥–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è, –∏–Ω—Ç–µ—Ä–µ—Å—ã —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ —Å–∫—Ä—ã—Ç—ã–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏.

–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
1. 5-7 –û—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º, –æ –∫–æ—Ç–æ—Ä—ã—Ö –≥–æ–≤–æ—Ä—è—Ç —É—á–∞—Å—Ç–Ω–∏–∫–∏ (–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã—Ö –∫ –Ω–∞–∏–º–µ–Ω–µ–µ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–º)
2. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã —É–∫–∞–∂–∏—Ç–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞
3. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã (–ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è/–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è)
4. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è

–°–û–û–ë–©–ï–ù–ò–Ø:
{messages}

–í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
{{
    "topics": [
        {{
            "name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
            "keywords": ["–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 1", "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 2", ...],
            "percentage": XX.X,
            "sentiment": "positive/negative/neutral",
            "description": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è"
        }},
        ...
    ]
}}
"""

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏
MONETIZATION_ANALYSIS_PROMPT = """
–ö–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –∏ –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª–∏, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ç–µ–º –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫:
{topics_json}

–†–∞–∑—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã:
1. 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤/—É—Å–ª—É–≥, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
2. –ù–∞–∏–±–æ–ª—å—à—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ (–ø–æ–¥–ø–∏—Å–∫–∞/—Ä–∞–∑–æ–≤–∞—è –æ–ø–ª–∞—Ç–∞/freemium –∏ —Ç.–¥.)
3. –û—Ü–µ–Ω–∫—É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Ö–æ–¥–∞ (–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π)
4. –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è)
5. –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–∫–æ—Ä–æ—Ç–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–¥–ª–∏–Ω–Ω—ã–π)

–í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
{{
    "monetization_strategies": [
        {{
            "topic": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
            "products": [
                {{
                    "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏",
                    "description": "–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏",
                    "model": "–ú–æ–¥–µ–ª—å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏",
                    "revenue_potential": "–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π",
                    "implementation_complexity": "–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è",
                    "timeframe": "–∫–æ—Ä–æ—Ç–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–¥–ª–∏–Ω–Ω—ã–π"
                }},
                ...
            ]
        }},
        ...
    ]
}}
"""

# –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω–∞
BUSINESS_PLAN_PROMPT = """
–ö–∞–∫ –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ç–µ–º –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫:
{topics_json}

–†–∞–∑—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã:
1. 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤/—É—Å–ª—É–≥, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
2. –ù–∞–∏–±–æ–ª—å—à—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ (–ø–æ–¥–ø–∏—Å–∫–∞/—Ä–∞–∑–æ–≤–∞—è –æ–ø–ª–∞—Ç–∞/freemium –∏ —Ç.–¥.)
3. –û—Ü–µ–Ω–∫—É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ—Ö–æ–¥–∞ (–Ω–∏–∑–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–≤—ã—Å–æ–∫–∏–π)
4. –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è)
5. –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–∫–æ—Ä–æ—Ç–∫–∏–π/—Å—Ä–µ–¥–Ω–∏–π/–¥–ª–∏–Ω–Ω—ã–π)

–í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
{{
    "business_plan": {{
        "executive_summary": {{
            "concept": "",
            "target_audience": "",
            "value_proposition": ""
        }},
        "market_analysis": {{
            "market_size": "",
            "trends": [],
            "competitors": []
        }},
        "product_description": {{
            "features": [],
            "usp": ""
        }},
        "business_model": {{
            "pricing": "",
            "sales_channels": []
        }},
        "marketing_strategy": {{
            "acquisition_channels": [],
            "retention_methods": []
        }},
        "operational_plan": {{
            "resources": [],
            "team": [],
            "timeline": {{}}
        }},
        "financial_forecast": {{
            "initial_investment": "",
            "breakeven": "",
            "roi": ""
        }},
        "risks": [
            {{
                "description": "",
                "mitigation": ""
            }}
        ]
    }}
}}
"""
